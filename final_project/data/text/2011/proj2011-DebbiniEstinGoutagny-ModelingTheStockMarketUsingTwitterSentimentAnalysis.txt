Introduction 

  
Team members: Daniel Debbini, Philippe Estin, Maxime Goutagny 
Supervisor: Mihai Surdeanu (with John Bauer) 
  
1     
 
This  project  is  based  on  Bollen,  Mao  and  Zengâ€™s  Twitter  mood  predicts  the  stock  market,  which 
shows  that  sentiment  extracted  from  Twitter  tweets  can  be  used  efficiently  to  enhance  forecasts  on 
the  direction  of  stock  market  moves  in  the  short  term  [1].  They  claim  an  87.6%  accuracy  in 
predicting  stock  market  moves  (up  or  down).  Our  objective  will  thus  be  to  implement  a  trading 
mechanism based on Twitter data and past stock price time series, and to compare the performance of 
Twitter-enhanced strategies with more traditional algorithmic strategies.    
 
2      Data 
 
Courtesy  of  Jure  Lescovic, we  have  a  sample  of  c.  480 million  tweets  from  June  2009  to December 
2009  [2].    Each  individual  tweet  is  composed  of  three  items:  the  time  stamp  of  the  tweet,  the  user 
who  posted  the  tweet,  and  the  text  body  of  the  tweet.    Further, we  have  time  series  data  of  the Dow 
Jones  Industrial Average  (DJIA),  the S&P 500, and  individual  stock prices during  the  second half of 
2009 (provided by our supervisor). 
 
3     
 
Below, we consider three traditional trading strategies. 
 

Trading Strategies 

CS 229, Autumn 2011 
Modeling the Stock Market Using Twitter Sentiment Analysis 

1.  X Filter rule.  If the price closes at least x% up, the strategy consist in opening a long position 
(buy)  and  hold  until  the  price moves  down  at  least  x%  from  a  subsequent  high.  In  this  case, 
you  go  short  (sell)  and  maintain  the  short  position  until  the  price  rises  at  least  x%  above  a 
ğ‘ğ‘(cid:1)(cid:1)(cid:1)
2.  Moving average rule. Let mat = (cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)
subsequent low. Moves of less than x% in either direction are ignored. 
, use the strategy to buy if pt > mat and to sell if pt 
3.  Resistance support rule. Buy if pt > max(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) ğ‘ğ‘(cid:1)(cid:1)(cid:1)  and sell if pt < min(cid:1)(cid:1)(cid:1)(cid:1)(cid:1) ğ‘ğ‘(cid:1)(cid:1)(cid:1) . 
< mat. The optimal L needs to be determined. 
 
Each of these strategies relies on a parameter.  We considered a set of parameters, fit each model to a 
training  set  (DJIA  prices  from  January  2000  to  October  2009)  and  tested  on  a  test  set  (DJIA  prices 
from November  2009  to December  2009).    The  results  of  the  best  training  fit  are  in  the  table  on  the 
next  page  (Table  1).   Correlation  between  the  training  performance  and  the  test  performance  is  low.  
This could be due  to  the non-stationarity of  the DJIA process and  suggests  that we may benefit  from 
a Twitter-enhanced strategy using additional information. 
TT rr aa dd ii nn gg  Â  Â  ss tt rr aa tt ee gg yy  Â  Â 
PP aa rr aa mm ee tt ee rr  Â  Â  AA nn nn uu aa ll  Â  Â  PP ee rr ff oo rr mm aa nn cc ee ,,  Â  Â 
AA nn nn uu aa ll  Â  Â 
Table 1. Traditional trading strategies: best fit over training set and test performance. 
TT rr aa ii nn ii nn gg  Â  Â 
PP ee rr ff oo rr mm aa nn cc ee ,,  Â  Â  TT ee ss tt  Â  Â 
0.00% Â 
8.37% Â 
4.84% Â 
X Â Filter Â 
Moving Â Average Â 
Lag Â = Â 1 Â 
-Â­â€2.78% Â 
-Â­â€32.61% Â 
Resistance Â Support Â 
Lag Â = Â 1 Â 
0.68% Â 
-Â­â€4.13% Â 

 

Initial Attempt 

4     
 
In  Bollen  et  al.,  two  sentiment  analysis  tools  are  applied  to  tweets   [1].    The  first  is  called 
OpinionFinder which  is  designed  to  identify whether  sentences  are  emotionally  positive  or  negative.  
The second is called GPOMS, which attempts to measure 6 mood dimensions: calm, alert, sure, vital, 
kind, and happy. 
 
As a first try to capture the sentiment of our  tweets, we used Alex Daviesâ€™ Twitter sentiment analysis 
word  list  [3].   For  two  sentiments, happy  and  sad,  the word  list gives  the  log probability of  the word 
and the sentiment.  More formally, for each word, we have an estimate of:  
log ğ‘ğ‘ ğ‘¤ğ‘¤ , ğ‘ ğ‘ 
ğ‘¤ğ‘¤ = ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘¤ğ‘‘ğ‘‘ , ğ‘ ğ‘  = ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  âˆˆ {â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ , ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  } 
,
 
 
Then,  Davies  proposes  a  method  for  estimating  the  probability  that  the  tweet  is  happy  given  the 
words  of  the  tweet,  assuming  the  prior  probabilities  of  each  sentiment  are  equal   [2]  (see  below), 
under NaÃ¯ve Bayes.  Assuming a tweet t is composed of words w: 
 ğ‘ğ‘ â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘¡ğ‘¡ = ğ‘ğ‘(â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ , ğ‘¡ğ‘¡)
ğ‘ğ‘ ğ‘¡ğ‘¡ â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘ğ‘(â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘)
=
ğ‘ğ‘ ğ‘¡ğ‘¡ â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ ğ‘ğ‘ â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ +  Â ğ‘ğ‘ ğ‘¡ğ‘¡ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  ğ‘ğ‘(ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  ) 
ğ‘ğ‘(ğ‘¡ğ‘¡)
1
  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â =  Â 
ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ ğ‘ğ‘ ğ‘¤ğ‘¤ , ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘  âˆ’ ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ğ‘™ ğ‘ğ‘ ğ‘¤ğ‘¤ , â„ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
1 + exp  Â 
(cid:1)âˆˆ(cid:1)
 
 
Thus,  using  the  Twitter  sentiment  word  list,  we  can  compute  the  probability  of  each  tweet  being 
happy or sad using the above formula.  We can then threshold this probability to determine if a tweet 
is happy, sad, or neutral. 
 
Tweets are split  into words, or â€œtokenized,â€ by separating white space.   In  this way, we still preserve 
emoticons such as â€œ:)â€ and â€œ<3â€, but tokens like â€œhappyâ€ and â€œhappy!â€ are different.   
 
Using  the  strategy  above,  we  parsed  the  tweets  and  then  performed  sentiment  analysis   using  a 
threshold of 0.5  (e.g.  if P(t, happy) > 0.5, classify  t as happy).   An excerpt of our  results  is  shown  in 
Table  2  (on  the  next  page).    It  seems  using  the  current word  list, we  are  classifying  almost  the  same 
fraction  of  tweets  per  day  as  happy/sad  (the  numbers  donâ€™t  add  to  100%  because  some  tweets  are 
composed of words not found  in our dictionary, and were designated â€œneutralâ€).    We also  tried using 
different thresholds (e.g. 0.7), but the results were extremely similar.  
 
Table  2.    Sentiment  analysis  performed  on  a  sample  of  tweets  using  Alex  Daviesâ€™  sentiment  word 
list, showing the DJIA return and a flag showing whether the market went up.  Threshold probability: 
NN uu mm bb ee rr  Â  Â 
PP ee rr cc ee nn tt aa gg ee  Â  Â 
PP ee rr cc ee nn tt aa gg ee  Â  Â 
0.5. 
oo ff  Â  Â  hh aa pp pp yy  Â  Â 
oo ff  Â  Â  ss aa dd  Â  Â 
oo ff  Â  Â 
DD JJ II AA  Â  Â 
tt ww ee ee tt ss  Â  Â 
tt ww ee ee tt ss  Â  Â 
TT ww ee ee tt ss  Â  Â 
rr ee tt uu rr nn  Â  Â 
DD aa tt ee  Â  Â 
 Â  Â  Â 1,055,053 Â  Â 
-Â­â€2.13% Â 
1.67% Â 
66.09% Â 
6/15/2009 Â 
-Â­â€1.25% Â 
6/16/2009 Â 
66.12% Â 
1.65% Â 
 Â  Â  Â  Â  Â 982,576 Â  Â 
 Â 
 Â 
 Â 
 Â 
 Â 

 Â  Â  Â  Â  Â  Â 864,568 Â  Â 
6/17/2009 Â 
66.79% Â 
1.63% Â 
-Â­â€0.09% Â 
 
6/18/2009 Â 
66.76% Â 
1.67% Â  819,349 Â  Â 
0.69% Â 
 
6/19/2009 Â 
66.38% Â 
1.56% Â  768,976 Â  Â 
-Â­â€0.19% Â 
 
 
 
 
Tokenization 
5     
 
The  first  step  in  extracting  better  information  from  Twitter was  to  improve  the  parsing  of  tweets,  or 
â€œtokenization.â€    Using  advice  from  Christopher  Pottsâ€™  sentiment  tutorial   [4],  we  improved  the 
tokenizer by recognizing the following special character strings: 
 

Â§ï‚§  Phone numbers 
Â§ï‚§  Emoticons 
Â§ï‚§  HTML tags, entities 
Â§ï‚§  Twitter usernames and hashtags 
Â§ï‚§  Websites 
Â§ï‚§  Words with hyphens, dashes, apostrophes, or underscores 
Â§ï‚§  Ellipses 
Â§ï‚§  Decimals and fractions 
Â§ï‚§  Words in all capital letters 

 
Some  of  the  above  special  cases  are  worth  discussing  below.    â€œHashtagsâ€  in  Twitter  are  words 
preceded  by  â€˜#â€™  which  mark  keywords  or  topics  in  a  tweet.     Words  in  all  capital  letters  usually 
convey heightened emotion, and thus we preserve words in all capital letters.  
 
One  final  preprocessing  step  we  used  per  Christopher  Potts  was  to  recognize  lengthening  by 
character  repetition  [4].    From  Potts,  lengthening  â€œis  a  reliable  indicator  of  heightened  emotionâ€¦ 
sequences  of  three  or more  identical  letters  in  a  row  are  basically  unattested  in  the  standard  lexicon, 
so  such  sequences  are  very  likely  to  be  lengthening.â€    Thus,  we  mapped  sequences  of  length  3  or 
greater to sequences of length 3.  For example: 
 
yaaaaaaaaaaaay   
becomes yaaay 
hahahahahaha    
becomes hahaha 
lololololololol  
becomes lololol 
 
We  did  not  shorten  punctuation  that  was  lengthened  (e.g.  five  multiple  exclamation  marks  in  a  row 
were treated as five distinct tokens). 
 
Using a tokenizer with the above competency, we parsed all tweets between July 20 09 and December 
2009.    The  next  step  is  to  extract  features  from  these  tokens  so  that  we  may  fit  a  model  to  predict 
stock market moves. 
 
 
 
 
 
 

 

Feature Selection and Mutual Information 

6     
 
Rather  than  relying  on  pre-trained  lexicons,  our  method  aimed  at  selecting  tokens  that  have  impact 
on  the  stock market  in a â€œnaturalâ€ and automatic  fashion. For each day, we compute  the occurrences 
of every token over the two previous days.  A token is said to have been â€œfrequentâ€ over the past two 
days if it is in the top 1,000 occurrences in the past two days.  Furthermore, for each day we compute 
the  daily  return  of  the  DJIA.    A  day  is  said  to  have  â€œlarge  variationâ€  if  the  absolute  DJIA  return  on 
ğ‘¦ğ‘¦ ((cid:1)) =  Â  ğŸ™ğŸ™{ Â (cid:2)(cid:1)(cid:3) Â (cid:1)  Â (cid:2)(cid:1)(cid:3) Â large Â variation}  
that day is greater than or equal to 1%.  Define the indicators: 
ğ‘¥ğ‘¥(cid:1)((cid:1)) =  Â  ğŸ™ğŸ™{ Â (cid:5)(cid:4)(cid:2)(cid:1)(cid:3) Â (cid:1)  Â (cid:1)(cid:2) Â (cid:2)(cid:5)(cid:1)(cid:4)(cid:7)(cid:1)(cid:3)(cid:6) Â (cid:2)(cid:4)(cid:1)(cid:3) Â (cid:2)(cid:1)(cid:4)(cid:3) Â (cid:1)(cid:1)(cid:1),(cid:1)(cid:1)(cid:1)}  
 
ğ‘ğ‘ ğ‘¥ğ‘¥(cid:1) , ğ‘¦ğ‘¦ log ğ‘ğ‘(ğ‘¥ğ‘¥(cid:1) , ğ‘¦ğ‘¦)
MI ğ‘¥ğ‘¥(cid:1) , ğ‘¦ğ‘¦ =  Â 
We can then compute the mutual information (MI) between xj and y [5]: 
ğ‘ğ‘ ğ‘¥ğ‘¥(cid:1) ğ‘ğ‘(ğ‘¦ğ‘¦)
(cid:1)(cid:1)âˆˆ{(cid:1),(cid:1)}
(cid:1)âˆˆ{(cid:1),(cid:1)}
The probabilities  in  the  above  formula  can be  estimated  according  to  their  empirical distributions on 
the training set. 
 
When  fitting  models  to  predict  the  DJIA  move  (up  or  down)  on  day  i,  the  feature  associated  with  a 
token  is  the  frequency  of  that  token  over  the  past  two  days  (number  of  occurrences  divided  by  total 
number  of  tokens  over  the  past  two  days).   However,  since we  have  such  a  large  number  of  tokens, 
we will only consider the tokens with the top k mutual information scores (see below for the selection 
of k). 
 
7      Models and Results 
 
Using  the  features  described  in  the  previous  section,  we  use   both  logistic  regression  and  a  support 
vector machine  (SVM)  to predict  the DJIA move  (up or down)  [6].   The  training  set was  the  first 80 
trading  days  between  June  and  December  2009,  the  test  set  was  the  last  57  trading  days.     For  the 
SVM, we used the Gaussian kernel shown below: 
ğ¾ğ¾ ğ‘¥ğ‘¥ , ğ‘§ğ‘§ = exp âˆ’ ğ‘¥ğ‘¥ âˆ’ ğ‘§ğ‘§ (cid:1)
= exp âˆ’ğ›¾ğ›¾ ğ‘¥ğ‘¥ âˆ’ ğ‘§ğ‘§ (cid:1)  
 
2ğœğœ (cid:1)
 
The  only  parameter  for  logistic  regression  is  k,  the  number  of  features  used  (see  previous  section).  
parameters  to  fit  are  ğ›¾ğ›¾ ,  C,  and  k.    Gamma  (ğ›¾ğ›¾ )  parameterizes  the  kernel  and  C  is  the  cost  parameter 
We  chose  k  based  on  5-fold  cross  validation  (k  =  5â€¦300),  yielding  k  =  55.    For  the  SVM,  the 
associated with the regularization of the SVM.    The results are shown in the below figures and table. 
 
Figure 1.   Sample  tokens used  in  final SVM model  (separated by commas).  :-), NEW, CANâ€™T, win, 
amazing,  LMAO,  $,  :(,  +,  MONEY,  GOD,  facebook,  bad,  friends,  #IRANELECTION,  summer, 
hahaha, news,  home, BUSINESS, love, JOB, TIMES, game, OFF, NICE, stop. 

We see on the graph at the right that on the test set (Nov, Dec 2009), the logistic regression strategy 
is superimposed with the Dow Jones, while the SVM strategy performs better. 

 

 
 
 

 
 
Figure 2. Performance of SVM and logistic regression models  
 
 
 
 

125

130

120

115

95

90

110

105

100

Dow	 Â Jones
SVM
Logistic 	 Â reg.

TT ee ss tt  Â  Â  SS ee tt  Â  Â   Â  Â 
Table 3. Performance 
 Â 
 Â  Â 
 Â  Â 
TT rr aa ii nn  Â  Â  TT ee ss tt  Â  Â  AA nn nn uu aa ll ii zz ee dd  Â  Â 
 Â 
RR ee tt uu rr nn  Â  Â 
EE rr rr oo rr  Â  Â  EE rr rr oo rr  Â  Â 
 Â 
39% Â 
SVM Â 
0.39 Â 
0.37 Â 
34% Â 
L.R. Â 
0.43 Â 
0.39 Â 
 
 
 
8      Conclusions and Future Work 
 
Twitter  is  a  treasure  trove of  information.   With millions of users posting  tweets  every  second,  there 
are opportunities to capture public sentiment/confidence/anxiety.  While the SVM performance (63% 
accuracy on the test set) is not as impressive as in Bollen et al. [1],  the annualized performance of the 
SVM is strong (39% annualized return).  The number of tokens used in the model (55) is reasonable. 
 
It  is yet  to be determined  if our accuracy can be sustained over a  longer period.   Our  test set  is  larger 
than Bollen et al.â€™s one (two months against 19 days) but the significance of the results would benefit 
from being extended to at least a year. Moreover, the Dow Jones performed well in the second half of 
2009:  the  strategy  should  be  tested  in  stress  times  as well. We  hope  our  framework  can  be  extended 
to  make  more  complex  predictions  about  the  DJIA  instead  of  a  binary  up/down  decision,  using 
softmax  regression  and/or  a  multi-class  SVM.    Furthermore,  we  wish  to  integrate  the  Twitter  data 
with  other more  traditional  statistical  features  into  one model   (e.g.  contrarian  strategies with Twitter 
information).    Additionally,  work  needs  to  be  done  to  analyze  the  effect  of  transaction  costs  and 
mistiming of trade execution (e.g. we may not be able to buy the DJIA during the open at yesterdayâ€™s 
closing price). 
 
9 
 

References 

[2] 

[1]  Bollen,  Mao,  and  Zeng.  Twitter  mood  predicts  the  stock  market.  Journal  of  Computational 
Science, 2 (1), March 2011. 
J.  Leskovec,  J.  Yang. Temporal  Variation  in  Online  Media.  ACM  International  Conference 
on Web Search and Data Mining (WSDM '11), 2011. 
[3]  Alex  Davies.    â€œA  word  list  for  sentiment  analysis  of  Twitter.â€  Web.  18  November  2011 
<http://alexdavies.net/2011/10/word-lists-for-sentiment-analysis-of-twitter/>. 
[4]  Christopher  Potts.    â€œSentiment  Symposium  Tutorial.â€    Web.    18  November  2011. 
<http://sentiment.christopherpotts.net>. 
[5]  Andrew Ng. â€œCS229 Lecure Notes. Regularization and model selection.â€ Web. 15 December 
2011. <http://cs229.stanford.edu/notes/cs229-notes5.pdf>. 
[6]  Chang, Chih-Chung and Lin, Chih-Jen. â€œLIBSVM â€“ A library for Support Vector Machines.â€  
Web.  15 December 2011.  <http://www.csie.ntu.edu.tw/~cjlin/libsvm/>. 

