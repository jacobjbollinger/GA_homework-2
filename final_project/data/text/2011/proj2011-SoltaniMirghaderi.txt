	 Â 
Seyed	 Â Reza	 Â Mir	 Â Ghaderi	 Â 	 Â 	 Â 
	 Â 
Nima	 Â Soltani	 Â 
	 Â 
	 Â 
Forecasting	 Â  Stock	 Â  Market	 Â  Behavior	 Â  Based	 Â 
on	 Â Public	 Â Sentiment	 Â 
It	 Â  is	 Â  believed	 Â  that	 Â  public	 Â  sentiment	 Â  is	 Â  correlated	 Â  with	 Â  the	 Â  behavior	 Â  of	 Â  the	 Â  stock	 Â  market	 Â  [1].	 Â  The	 Â 
general	 Â  objective	 Â  of	 Â  the	 Â  project	 Â  is	 Â  to	 Â  characterize	 Â  this	 Â  correlation	 Â  and	 Â  use	 Â  it	 Â  to	 Â  predict	 Â  the	 Â  future	 Â 
Project	 Â Overview	 Â 
behavior	 Â  of	 Â  the	 Â market.	 Â We	 Â  assume	 Â  that	 Â  people	 Â  express	 Â  their	 Â mood	 Â  in	 Â  their	 Â  Twitter	 Â  posts	 Â  (tweets)	 Â 
and	 Â approach	 Â  the	 Â problem	 Â by	 Â performing	 Â  large-Â­â€scale	 Â analysis	 Â on	 Â  these	 Â  tweets.	 Â The	 Â ultimate	 Â goal	 Â of	 Â 
this	 Â project	 Â  is	 Â  to	 Â predict	 Â how	 Â  the	 Â market	 Â will	 Â behave	 Â  tomorrow	 Â given	 Â a	 Â  large	 Â  set	 Â of	 Â  tweets	 Â over	 Â  the	 Â 
past	 Â few	 Â days.	 Â 	 Â 	 Â 
Previous	 Â  work	 Â  by	 Â  Bollen	 Â  et	 Â  al.	 Â  [1]	 Â  uses	 Â  sentiment	 Â  analysis	 Â  tools	 Â  in	 Â  conjunction	 Â  with	 Â  a	 Â  non-Â­â€linear	 Â 
model	 Â  (Self-Â­â€organizing	 Â  Fuzzy	 Â  Neural	 Â  Network)	 Â  to	 Â  predict	 Â  the	 Â  changes	 Â  in	 Â  Dow	 Â  Jones	 Â  Industrial	 Â 
Average	 Â  (DJIA).	 Â  Here	 Â  we	 Â  design	 Â  and	 Â  build	 Â  the	 Â  learning	 Â  and	 Â  prediction	 Â  system	 Â  making	 Â  only	 Â  basic	 Â 
assumptions	 Â  about	 Â  the	 Â  relationship	 Â  between	 Â  tweets	 Â  and	 Â  the	 Â  market.	 Â  We	 Â  do	 Â  not	 Â  assume	 Â  that	 Â  the	 Â 
moods	 Â of	 Â the	 Â tweets	 Â are	 Â their	 Â only	 Â important	 Â feature,	 Â and	 Â instead	 Â look	 Â for	 Â informative	 Â features	 Â that	 Â 
might	 Â  not	 Â  be	 Â  specifically	 Â  mood-Â­â€related.	 Â  The	 Â  reason	 Â  for	 Â  doing	 Â  so	 Â  is	 Â  to	 Â  give	 Â  enough	 Â  freedom	 Â  to	 Â  the	 Â 
algorithm	 Â itself	 Â to	 Â determine	 Â which	 Â words	 Â are	 Â most	 Â pertinent	 Â to	 Â the	 Â stock	 Â values.	 Â 	 Â 
Data	 Â When	 Â  starting	 Â  this	 Â  project,	 Â  we	 Â  had	 Â  data	 Â  for	 Â  6	 Â  months	 Â  in	 Â  2009.	 Â  This	 Â  included	 Â  data	 Â  obtained	 Â  from	 Â 
Twitter	 Â over	 Â the	 Â course	 Â of	 Â the	 Â 6	 Â months	 Â as	 Â well	 Â as	 Â daily	 Â stock	 Â values.	 Â 	 Â 
We	 Â  had	 Â  training	 Â  data	 Â  in	 Â  the	 Â  form	 Â  of	 Â  ~60GB	 Â  of	 Â  Twitter	 Â  posts	 Â  [2]	 Â  over	 Â  the	 Â  timespan	 Â  of	 Â  June	 Â  to	 Â 
December	 Â  of	 Â  2009.	 Â  Given	 Â  the	 Â  large	 Â  volume	 Â  of	 Â  data,	 Â we	 Â  needed	 Â  to	 Â  format	 Â  the	 Â  data	 Â  into	 Â  a	 Â  consistent	 Â 
Tweets	 Â 
format.	 Â We	 Â first	 Â tried	 Â using	 Â Stanfordâ€™s	 Â NLP	 Â Java	 Â application	 Â to	 Â parse	 Â the	 Â documents	 Â and	 Â get	 Â the	 Â data	 Â 
in	 Â  a	 Â  readable	 Â  format.	 Â  The	 Â  advantages	 Â  of	 Â  this	 Â  approach	 Â  was	 Â  that	 Â  it	 Â  was	 Â  already	 Â  written	 Â  and	 Â 
debugged,	 Â as	 Â well	 Â as	 Â containing	 Â features	 Â like	 Â grouping	 Â words	 Â together	 Â by	 Â their	 Â root	 Â word.	 Â The	 Â main	 Â 
disadvantage	 Â of	 Â this	 Â approach,	 Â however,	 Â was	 Â that	 Â  it	 Â took	 Â much	 Â more	 Â time	 Â and	 Â space	 Â to	 Â run	 Â than	 Â we	 Â 
could	 Â allow	 Â in	 Â our	 Â large-Â­â€scale	 Â problem.	 Â 	 Â 
Thus	 Â  we	 Â  decided	 Â  to	 Â  write	 Â  our	 Â  own	 Â  parser/tokenizer.	 Â  By	 Â  use	 Â  of	 Â  regular	 Â  expressions	 Â  in	 Â  Python,	 Â  we	 Â 
filtered	 Â  out	 Â  tweets	 Â  with	 Â  non-Â­â€English	 Â  letters,	 Â  tokenized	 Â  URLs,	 Â  numbers,	 Â  twitter	 Â  usernames,	 Â  and	 Â 
emoticons,	 Â  converted	 Â  everything	 Â  to	 Â  lowercase	 Â  and	 Â  removed	 Â  all	 Â  punctuation.	 Â We	 Â  then	 Â  ran	 Â  a	 Â  Python	 Â 
stemming	 Â  tool	 Â  (stemming	 Â  1.0)	 Â  to	 Â  remove	 Â  the	 Â  suffixes	 Â  and	 Â  attempt	 Â  to	 Â  find	 Â  the	 Â  root	 Â  word	 Â  of	 Â  the	 Â 
words	 Â in	 Â the	 Â tweets.	 Â Also,	 Â since	 Â each	 Â tweet	 Â has	 Â a	 Â 140	 Â character	 Â maximum,	 Â we	 Â decided	 Â to	 Â make	 Â each	 Â 
tweet	 Â 50	 Â words	 Â long,	 Â where	 Â we	 Â truncated	 Â the	 Â tweet	 Â if	 Â it	 Â contained	 Â more	 Â than	 Â 50	 Â words	 Â and	 Â padded	 Â 
the	 Â  tweet	 Â with	 Â NULL	 Â words	 Â  if	 Â  it	 Â  contained	 Â  less.	 Â We	 Â  ran	 Â  this	 Â  script	 Â  on	 Â  all	 Â  our	 Â  data,	 Â  training	 Â  and	 Â  test	 Â 
data,	 Â so	 Â we	 Â can	 Â read	 Â words	 Â with	 Â consistent	 Â formatting.	 Â 

	 Â 
Seyed	 Â Reza	 Â Mir	 Â Ghaderi	 Â 	 Â 	 Â 
	 Â 
Nima	 Â Soltani	 Â 
	 Â 
	 Â 
For	 Â  all	 Â  the	 Â  learning	 Â  algorithms	 Â  except	 Â  linear	 Â  regression,	 Â  we	 Â  used	 Â  daily	 Â  open/close	 Â  values	 Â  of	 Â  Dow	 Â 
Jones	 Â  Industrial	 Â Average	 Â  (DJIA)	 Â  from	 Â  June	 Â 12	 Â 2009-Â­â€Dec	 Â 30	 Â 2009.	 Â For	 Â  the	 Â  linear	 Â  regression	 Â we	 Â used	 Â 
Stocks	 Â 
DJIA	 Â  hourly	 Â  values	 Â  for	 Â  the	 Â  same	 Â  period	 Â  (obtained	 Â  from	 Â  Price-Â­â€Data).	 Â  We	 Â  tried	 Â  different	 Â  labeling	 Â 
definitions	 Â  to	 Â  form	 Â  the	 Â  classification	 Â  problems.	 Â  In	 Â  particular	 Â  for	 Â  one	 Â  bit	 Â  representation	 Â  of	 Â  the	 Â  state	 Â 
of	 Â the	 Â stock	 Â market	 Â following	 Â day	 Â ğ‘¡ 	 Â we	 Â tried	 Â 	 Â 
ğ‘¦ = 1 ğ‘‚ğ‘ğ‘’ğ‘›ğ‘›ğ‘–ğ‘›ğ‘” ğ‘¡ + 1 > ğ¶ğ‘™ğ‘œğ‘ ğ‘–ğ‘›ğ‘” ğ‘¡
	 Â 
ğ‘¦ = 1 ğ¶ğ‘™ğ‘œğ‘ ğ‘–ğ‘›ğ‘” ğ‘¡ + 1 > ğ¶ğ‘™ğ‘œğ‘ ğ‘–ğ‘›ğ‘” ğ‘¡
	 Â 
ğ‘¦ = 1 ğ¶ğ‘™ğ‘œğ‘ ğ‘–ğ‘›ğ‘” ğ‘¡ + 1 > ğ‘‚ğ‘ğ‘’ğ‘›ğ‘›ğ‘–ğ‘›ğ‘” ğ‘¡ + 1 	 Â 
Similar	 Â approach	 Â was	 Â tried	 Â for	 Â the	 Â growth	 Â computation	 Â in	 Â Match/Score	 Â algorithm.	 Â 	 Â 
One	 Â of	 Â the	 Â main	 Â differences	 Â between	 Â our	 Â work	 Â and	 Â the	 Â original	 Â work	 Â [1]	 Â is	 Â that	 Â we	 Â try	 Â to	 Â generalize	 Â 
the	 Â feature	 Â set.	 Â The	 Â original	 Â feature	 Â set	 Â used	 Â 7	 Â features	 Â from	 Â two	 Â sentiment	 Â analysis	 Â tools:	 Â a	 Â general	 Â 
Feature	 Â Selection	 Â 
positive/negative	 Â classification,	 Â and	 Â mood	 Â states	 Â in	 Â 6	 Â dimensions	 Â of	 Â calm,	 Â alert,	 Â sure,	 Â vital,	 Â kind	 Â and	 Â 
happy.	 Â Also,	 Â  they	 Â  took	 Â out	 Â  tweets	 Â  that	 Â did	 Â not	 Â explicitly	 Â express	 Â  the	 Â authorâ€™s	 Â mood.	 Â Our	 Â goal	 Â  in	 Â  this	 Â 
project,	 Â  however,	 Â  was	 Â  to	 Â  remove	 Â  these	 Â  constraints	 Â  and	 Â  allow	 Â  the	 Â  algorithm	 Â  to	 Â  select	 Â  the	 Â  most	 Â 
informative	 Â content	 Â in	 Â the	 Â day.	 Â 	 Â 
To	 Â  select	 Â  the	 Â  features,	 Â  we	 Â  found	 Â  the	 Â  statistics	 Â  of	 Â  the	 Â  words	 Â  appearing	 Â  every	 Â  hour.	 Â  Despite	 Â  having	 Â 
tens	 Â of	 Â gigabytes	 Â worth	 Â of	 Â tweets,	 Â we	 Â realistically	 Â had	 Â only	 Â 100	 Â training	 Â data	 Â points,	 Â so	 Â we	 Â could	 Â not	 Â 
Clustering	 Â 
confidently	 Â train	 Â a	 Â hypothesis	 Â with	 Â more	 Â than	 Â 5-Â­â€10	 Â input	 Â features.	 Â In	 Â order	 Â to	 Â make	 Â good	 Â use	 Â of	 Â the	 Â 
data,	 Â we	 Â  then	 Â used	 Â a	 Â k-Â­â€means	 Â clustering	 Â algorithm	 Â  to	 Â  find	 Â words	 Â  that	 Â varied	 Â consistently	 Â with	 Â each	 Â 
other	 Â so	 Â as	 Â to	 Â maximize	 Â the	 Â number	 Â of	 Â useful	 Â words	 Â to	 Â use	 Â as	 Â features.	 Â 	 Â 
For	 Â  the	 Â  clustering,	 Â  we	 Â  tried	 Â  clustering	 Â  based	 Â  two	 Â  different	 Â  sets	 Â  of	 Â  features:	 Â  (1)	 Â  the	 Â  ratio	 Â  of	 Â  the	 Â 
hourly	 Â word	 Â  count	 Â  to	 Â  the	 Â  total	 Â  hourly	 Â word	 Â  count,	 Â  or	 Â  (2)	 Â  the	 Â  ratio	 Â  of	 Â  the	 Â  hourly	 Â word	 Â  count	 Â  to	 Â  the	 Â 
total	 Â  count	 Â  of	 Â  the	 Â  word	 Â  across	 Â  all	 Â  time.	 Â  Running	 Â  the	 Â  clustering	 Â  algorithm	 Â  on	 Â  these	 Â  two	 Â  sets	 Â 
produced	 Â  fundamentally	 Â  different	 Â  features.	 Â  The	 Â  clusters	 Â  using	 Â  (1)	 Â  lumped	 Â  words	 Â  of	 Â  nearly	 Â  equal	 Â 
frequency	 Â  together.	 Â  This	 Â  did	 Â  a	 Â  good	 Â  job	 Â  of	 Â  clustering	 Â  words	 Â  that	 Â  are	 Â  extremely	 Â  common	 Â  and	 Â  have	 Â 
seemingly	 Â  no	 Â  correlation	 Â  with	 Â  the	 Â  market,	 Â  such	 Â  as	 Â  â€œtheâ€,	 Â  â€œofâ€,	 Â  â€œandâ€,	 Â  etc.	 Â  The	 Â  clusters	 Â  using	 Â  (2)	 Â 
lumped	 Â words	 Â  that	 Â  fluctuated	 Â  similarly	 Â  together,	 Â  regardless	 Â  of	 Â  the	 Â  number	 Â  of	 Â  times	 Â  they	 Â  appeared	 Â 
each	 Â  hour.	 Â  The	 Â  common	 Â  words	 Â  were	 Â  still	 Â  mainly	 Â  lumped	 Â  together,	 Â  but	 Â  there	 Â  were	 Â  some	 Â  mixed	 Â  in	 Â 
other	 Â sets.	 Â It	 Â was	 Â also	 Â interesting	 Â to	 Â see	 Â that	 Â in	 Â both	 Â cases,	 Â the	 Â algorithm	 Â assigned	 Â Spanish	 Â words	 Â to	 Â 
their	 Â  own	 Â  cluster.	 Â  After	 Â  coming	 Â  up	 Â  with	 Â  100	 Â  clusters,	 Â  for	 Â  each	 Â  cluster	 Â ğ¶! 	 Â we	 Â  then	 Â  calculated	 Â  the	 Â 
mutual	 Â  information	 Â  as	 Â  	 Â ğ¼(1 #ğ¶! /ğ‘› > ğ¸ #ğ¶! /ğ‘› ; ğ‘¦! ),	 Â  where	 Â ğ‘› 	 Â is	 Â  the	 Â  number	 Â  of	 Â  words	 Â  that	 Â  day.	 Â  We	 Â 
then	 Â sorted	 Â them	 Â according	 Â to	 Â mutual	 Â information	 Â and	 Â took	 Â the	 Â 8	 Â most	 Â informative	 Â clusters.	 Â 	 Â 

	 Â 
Seyed	 Â Reza	 Â Mir	 Â Ghaderi	 Â 	 Â 	 Â 
	 Â 
Nima	 Â Soltani	 Â 
	 Â 
	 Â 
Surprisingly,	 Â  this	 Â  method	 Â  of	 Â  calculation	 Â  only	 Â  made	 Â  a	 Â  1%	 Â  change	 Â  in	 Â  the	 Â  performance	 Â  of	 Â  our	 Â 
algorithm.	 Â  In	 Â  retrospect,	 Â  the	 Â  clustering	 Â  strategy	 Â  should	 Â  have	 Â  somehow	 Â  considered	 Â  the	 Â  mutual	 Â 
information	 Â more	 Â explicitly.	 Â Also,	 Â one	 Â problem	 Â with	 Â this	 Â method	 Â was	 Â that	 Â it	 Â was	 Â almost	 Â chaotic.	 Â Any	 Â 
small	 Â perturbation	 Â  in	 Â  the	 Â algorithm	 Â parameters	 Â changed	 Â  the	 Â resulting	 Â clusters.	 Â Clustering	 Â based	 Â on	 Â 
a	 Â more	 Â robust	 Â feature	 Â of	 Â the	 Â word	 Â also	 Â could	 Â have	 Â helped.	 Â 
The	 Â  inputs	 Â  to	 Â  the	 Â  algorithms	 Â  were	 Â  counters	 Â  of	 Â  clusters	 Â  with	 Â  daily	 Â  and	 Â  hourly	 Â  resolutions	 Â 
ğ‘‹!"# , ğ‘‹!!"# ,	 Â  and	 Â  the	 Â  stock	 Â  values/indicators.	 Â  In	 Â  all	 Â  the	 Â  models	 Â  we	 Â  used	 Â  the	 Â  tweets	 Â  of	 Â  one	 Â  day	 Â  to	 Â 
Market	 Â Behavior	 Â Learning/Inference	 Â Models	 Â 
predict	 Â  the	 Â  stock	 Â  behavior	 Â  on	 Â  the	 Â  next	 Â  day.	 Â  In	 Â  all	 Â  the	 Â  algorithms,	 Â we	 Â  used	 Â  70%	 Â  of	 Â  data	 Â  (tweets	 Â  and	 Â 
DJIA	 Â up/down	 Â indicator)	 Â for	 Â training	 Â and	 Â the	 Â rest	 Â for	 Â testing.	 Â We	 Â tried	 Â 4	 Â different	 Â algorithms:	 Â NaÃ¯ve	 Â 
Bayes,	 Â SVM,	 Â Linear	 Â Regression	 Â and	 Â our	 Â own	 Â heuristic	 Â algorithm	 Â Score/Match.	 Â 
Given	 Â  its	 Â  fast	 Â  running	 Â  time,	 Â  NaÃ¯ve	 Â  Bayes	 Â  was	 Â  our	 Â  best	 Â  choice	 Â  as	 Â  the	 Â  baseline	 Â  algorithm.	 Â  We	 Â 
predicted	 Â the	 Â probability	 Â of	 Â each	 Â cluster	 Â given	 Â each	 Â label	 Â and	 Â the	 Â label	 Â prior	 Â as	 Â 
NaÃ¯ve	 Â Bayes	 Â Learning	 Â 
	 Â 

	 Â 

P(Cluster i | y = b ) =

NumTrainingDays
1{yt = b} X day (t , i )
âˆ‘
t =1
NumTrainingDays
NumClusters
X day (t , j )
1{yt = b}
âˆ‘
âˆ‘
t =1
j =1
NumTrainingDays
1{yt = b}
âˆ‘
and	 Â used	 Â the	 Â following	 Â rule	 Â to	 Â predict	 Â the	 Â days	 Â in	 Â testing	 Â data	 Â 
P( y = b ) =
t =1
NumTrainingDays
NumClusters
â§
â«
P(Cluster i | y = 1) Xday ( t ,i )
P y = 1
âˆ
(
)
âªâª
âªâª
	 Â 
Ë†yt = 1
â‰¥ 1
i=1
â¨
â¬
NumClusters
P y = 0
P(Cluster i | y = 0 ) Xday ( t ,i )
Given	 Â  the	 Â  large	 Â occurrence	 Â of	 Â each	 Â word	 Â and	 Â each	 Â class	 Â (up/down)	 Â we	 Â did	 Â not	 Â need	 Â ant	 Â smoothing.	 Â 
âˆ
(
)
âª
âª
Given	 Â  the	 Â  relatively	 Â  large	 Â  level	 Â  of	 Â  error	 Â  (see	 Â  the	 Â  error	 Â  table)	 Â  and	 Â  the	 Â  strong	 Â  assumptions	 Â  made	 Â  in	 Â 
âª
âª
â©
â­
i=1
NaÃ¯ve	 Â Bayes,	 Â we	 Â decided	 Â to	 Â apply	 Â a	 Â discriminative	 Â algorithm	 Â as	 Â the	 Â next	 Â step.	 Â 	 Â 
The	 Â  feature	 Â  vectors	 Â  we	 Â  used	 Â  for	 Â  SVM	 Â  were	 Â  normalized	 Â  to	 Â  give	 Â  the	 Â  relative	 Â  occurrence	 Â  of	 Â  each	 Â 	 Â 
cluster	 Â  in	 Â  a	 Â  day.	 Â  We	 Â  used	 Â  cvx	 Â  for	 Â  convex	 Â  optimization	 Â  and	 Â  set	 Â  the	 Â  regularization	 Â  factor	 Â ğ¶ = 1.	 Â  To	 Â 
Support	 Â Vector	 Â Machine	 Â 

	 Â 
Seyed	 Â Reza	 Â Mir	 Â Ghaderi	 Â 	 Â 	 Â 
	 Â 
Nima	 Â Soltani	 Â 
	 Â 
	 Â 
avoid	 Â  overfitting	 Â  we	 Â  turned	 Â  the	 Â  real	 Â  frequency	 Â  vector	 Â  to	 Â  a	 Â  binary	 Â  vector	 Â  showing	 Â  whether	 Â  each	 Â 
frequency	 Â  is	 Â  higher	 Â  or	 Â  lower	 Â  than	 Â  its	 Â  average	 Â  value	 Â  over	 Â  all	 Â  the	 Â  training	 Â  days.	 Â  O	 Â  can	 Â  see	 Â  slight	 Â 
improvement	 Â  (see	 Â  the	 Â  error	 Â  table)	 Â  as	 Â  compared	 Â  to	 Â NaÃ¯ve	 Â  Bayes.	 Â However,	 Â  the	 Â  error	 Â was	 Â  still	 Â  large.	 Â 
We	 Â  decided	 Â  that	 Â  problem	 Â  may	 Â  come	 Â  from	 Â  is	 Â  the	 Â  loss	 Â  of	 Â  valuable	 Â  information	 Â  by	 Â  discretizing	 Â  the	 Â 
stock	 Â value.	 Â Therefore	 Â we	 Â used	 Â a	 Â linear	 Â regression	 Â model.	 Â 
We	 Â tried	 Â to	 Â predict	 Â the	 Â value	 Â of	 Â the	 Â stock	 Â by	 Â running	 Â a	 Â linear	 Â regression	 Â on	 Â 	 Â 	 Â 
â€¢  The	 Â stock	 Â value	 Â in	 Â the	 Â past	 Â few	 Â hours	 Â 
Linear	 Â Regression	 Â 
â€¢  The	 Â average	 Â stock	 Â value	 Â in	 Â the	 Â past	 Â day	 Â 
â€¢  The	 Â hourly	 Â count	 Â of	 Â clusters	 Â in	 Â the	 Â tweet	 Â data	 Â of	 Â the	 Â past	 Â 24	 Â hours	 Â 
We	 Â considered	 Â predicting	 Â the	 Â afternoon	 Â stock	 Â value	 Â 	 Â (to	 Â have	 Â data	 Â for	 Â the	 Â past	 Â few	 Â hours).	 Â The	 Â result	 Â 
indicated	 Â  that	 Â  with	 Â  the	 Â  size	 Â  of	 Â  data	 Â  we	 Â  had,	 Â  linear	 Â  regression	 Â  was	 Â  prone	 Â  to	 Â  overfitting.	 Â  While	 Â  the	 Â 
training	 Â error	 Â was	 Â very	 Â low,	 Â we	 Â got	 Â orders	 Â of	 Â magnitude	 Â higher	 Â testing	 Â error.	 Â 
As	 Â explained	 Â in	 Â the	 Â SVM	 Â section,	 Â we	 Â can	 Â obtain	 Â a	 Â binary	 Â feature	 Â vector	 Â by	 Â discretizing	 Â the	 Â frequency	 Â 
vector.	 Â Doing	 Â  so,	 Â we	 Â get	 Â binary	 Â vectors	 Â of	 Â  length	 Â 8.	 Â To	 Â each	 Â vector	 Â we	 Â assigned	 Â a	 Â  score	 Â equal	 Â  to	 Â  the	 Â 
Our	 Â Own	 Â Heuristic:	 Â Score/Match	 Â Algorithm	 Â 
stock	 Â  growth	 Â  following	 Â  that	 Â  vector	 Â  (following	 Â  that	 Â  day).	 Â  If	 Â  one	 Â  vector	 Â  happened	 Â  multiple	 Â  times	 Â  in	 Â 
our	 Â  data	 Â we	 Â  assigned	 Â  the	 Â  average	 Â  of	 Â  the	 Â  stock	 Â  growth	 Â  (negative	 Â  if	 Â  decline)	 Â  values.	 Â We	 Â  then	 Â  labeled	 Â 
the	 Â  testing	 Â  sequences	 Â  by	 Â  comparing	 Â  their	 Â  feature	 Â  vector	 Â  to	 Â  the	 Â  ones	 Â  in	 Â  the	 Â  training	 Â  set,	 Â  finding	 Â  the	 Â 
ones	 Â  with	 Â  minimum	 Â  Hamming	 Â  distance	 Â  and	 Â  finally	 Â  comparing	 Â  the	 Â  average	 Â  score	 Â  values	 Â  of	 Â  the	 Â 
closest	 Â  vectors	 Â  to	 Â  0	 Â  (see	 Â  the	 Â  diagram).	 Â  This	 Â  algorithm	 Â  (which	 Â was	 Â  inspired	 Â  by	 Â minimum	 Â  Hamming	 Â 
distance	 Â  decoding	 Â  in	 Â  communication)	 Â  lead	 Â  to	 Â  an	 Â  error	 Â  which	 Â  was	 Â  lower	 Â  than	 Â  all	 Â  the	 Â  other	 Â 
classification	 Â algorithms.	 Â 

	 Â 

Results	 Â NaÃ¯ve	 Â Bayes	 Â 
Training	 Â Error38%	 Â 
Test	 Â Error	 Â 42%	 Â 
	 Â 

Linear	 Â SVM	 Â 
Training	 Â  Error	 Â  40%	 Â 
Test	 Â Error	 Â 40%	 Â 

Linear	 Â Regression	 Â 
Training	 Â MSE	 Â 161	 Â 
Test	 Â MSE	 Â 140000	 Â 

Score/Match	 Â 
Test	 Â Error	 Â 37.5%	 Â 

	 Â 
Seyed	 Â Reza	 Â Mir	 Â Ghaderi	 Â 	 Â 	 Â 
	 Â 
Nima	 Â Soltani	 Â 
	 Â 
	 Â 
Although	 Â  we	 Â  did	 Â  not	 Â  meet	 Â  the	 Â  performance	 Â  of	 Â  the	 Â  original	 Â  paper,	 Â  we	 Â  were	 Â  solving	 Â  a	 Â  slightly	 Â 
different	 Â  problem.	 Â  The	 Â  result	 Â  of	 Â  37.5%	 Â  accuracy	 Â  is	 Â  quite	 Â  good	 Â  considering	 Â  the	 Â  generality	 Â  of	 Â  the	 Â 
Conclusions	 Â 
assumptions	 Â  we	 Â  were	 Â  making	 Â  and	 Â  the	 Â  limited	 Â  training	 Â  data	 Â  that	 Â  we	 Â  had	 Â  (the	 Â  original	 Â  paper	 Â  had	 Â  9	 Â 
monthsâ€™	 Â worth	 Â of	 Â training	 Â data).	 Â 
There	 Â  is	 Â  room	 Â  for	 Â  improvement	 Â  and	 Â  we	 Â  are	 Â  interested	 Â  in	 Â  pursuing	 Â  research	 Â  on	 Â  this	 Â  topic.	 Â  The	 Â 
following	 Â are	 Â potential	 Â methods	 Â of	 Â improvement	 Â that	 Â we	 Â plan	 Â to	 Â use:	 Â 
Future	 Â Improvements	 Â 
Due	 Â to	 Â the	 Â scarcity	 Â of	 Â data	 Â  in	 Â our	 Â project,	 Â we	 Â tried	 Â to	 Â maximize	 Â the	 Â amount	 Â of	 Â data	 Â we	 Â could	 Â use	 Â  for	 Â 
training,	 Â but	 Â one	 Â problem	 Â was	 Â that	 Â we	 Â did	 Â not	 Â update	 Â our	 Â training	 Â data	 Â once	 Â we	 Â tested	 Â it.	 Â Testing	 Â on	 Â 
Retrain	 Â with	 Â a	 Â sliding	 Â window	 Â 
a	 Â finite	 Â window	 Â of	 Â the	 Â past	 Â could	 Â provide	 Â a	 Â more	 Â realistic	 Â dataset.	 Â 	 Â 
The	 Â  mutual	 Â  information	 Â  will	 Â  increase	 Â  even	 Â  more	 Â  with	 Â  the	 Â  joint	 Â  distribution	 Â  of	 Â  the	 Â  words	 Â  in	 Â  the	 Â 
cluster	 Â as	 Â opposed	 Â to	 Â the	 Â distribution	 Â of	 Â the	 Â union	 Â of	 Â the	 Â words.	 Â  	 Â Intuitively,	 Â this	 Â should	 Â correspond	 Â 
Use	 Â pairs	 Â of	 Â words	 Â as	 Â features	 Â 
more	 Â to	 Â a	 Â contextual	 Â clustering	 Â of	 Â the	 Â words,	 Â as	 Â opposed	 Â to	 Â purely	 Â coincidental.	 Â 
The	 Â  current	 Â  system	 Â  only	 Â  considers	 Â  the	 Â  change	 Â  as	 Â  an	 Â  up/down	 Â  quantity	 Â  and	 Â  does	 Â  not	 Â  distinguish	 Â  a	 Â 
1%	 Â  change	 Â  in	 Â  the	 Â  market	 Â  from	 Â  a	 Â  10%	 Â  change.	 Â  We	 Â  could	 Â  make	 Â  a	 Â  ternary	 Â  system	 Â  where	 Â  we	 Â  add	 Â  an	 Â 
Use	 Â a	 Â different	 Â classification	 Â structure	 Â 
â€œinsignificant	 Â changeâ€	 Â level,	 Â corresponding	 Â to	 Â a	 Â -Â­â€1%	 Â to	 Â 1%	 Â change.	 Â Alternatively	 Â we	 Â could	 Â still	 Â use	 Â a	 Â 
binary	 Â classification	 Â but	 Â only	 Â train	 Â using	 Â data	 Â points	 Â corresponding	 Â to	 Â large	 Â changes.	 Â 
We	 Â  would	 Â  like	 Â  to	 Â  sincerely	 Â  thank	 Â  Ali	 Â  Reza	 Â  Sharafat,	 Â  who	 Â  helped	 Â  with	 Â  the	 Â  initial	 Â  bring-Â­â€up	 Â  of	 Â  the	 Â 
project	 Â  and	 Â  provided	 Â  advice	 Â  on	 Â  Python	 Â  coding	 Â  later	 Â  on	 Â  after	 Â  he	 Â  withdrew	 Â  from	 Â  the	 Â  course.	 Â  We	 Â 
Acknowledgements	 Â 
would	 Â also	 Â like	 Â to	 Â thank	 Â Mihai	 Â Surdeanu	 Â and	 Â John	 Â Bauer	 Â for	 Â proposing	 Â the	 Â topic.	 Â 
[1]	 Â 	 Â J.	 Â  Bollen,	 Â  H.	 Â  Maoa	 Â  and	 Â  X.	 Â  Zeng,	 Â  "Twitter	 Â  mood	 Â  predicts	 Â  the	 Â  stock	 Â  market,"	 Â  Journal	 Â  of	 Â 
Computational	 Â Science,	 Â pp.	 Â 1-Â­â€8,	 Â 2011.	 Â 	 Â 
Works	 Â Cited	 Â 
[2]	 Â 	 Â J.	 Â  Yang	 Â  and	 Â  J.	 Â  Leskovec,	 Â  "Patterns	 Â  of	 Â  Temporal	 Â  Variation	 Â  in	 Â  Online	 Â Media,"	 Â  in	 Â  ACM	 Â  International	 Â 
Conference	 Â on	 Â Web	 Â Search	 Â and	 Â Data	 Â Minig	 Â (WSDM),	 Â Hong	 Â Kong,	 Â 2011.	 Â 	 Â 
	 Â 
	 Â 
	 Â 

