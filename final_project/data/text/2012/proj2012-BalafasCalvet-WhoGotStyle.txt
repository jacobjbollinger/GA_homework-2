Who	 Â Got	 Â Style?	 Â 
	 Â CS	 Â 229	 Â Final	 Â Project	 Â 
Fall	 Â Quarter	 Â 2012	 Â 
Learning	 Â to	 Â recognize	 Â literary	 Â styles	 Â 
	 Â Konstantinos	 Â Balafas	 Â 
Simon	 Â Calvet	 Â 
	 Â 
	 Â Most	 Â  well	 Â  known	 Â  authors	 Â  have	 Â  a	 Â  distinct	 Â  writing	 Â  style	 Â  that	 Â  renders	 Â  their	 Â  work	 Â 
unique.	 Â  For	 Â  the	 Â  erudite	 Â  reader,	 Â  that	 Â  style	 Â  is	 Â  quite	 Â  obvious	 Â  and	 Â  it	 Â  is	 Â  not	 Â  hard	 Â  to	 Â  tell	 Â 
Introduction	 Â 
between	 Â  Jane	 Â  Austen	 Â  and	 Â  Charles	 Â  Dickens	 Â  or	 Â  Leo	 Â  Tolstoy,	 Â  to	 Â  name	 Â  a	 Â  few.	 Â  That	 Â 
distinction,	 Â however,	 Â  requires	 Â  skill	 Â and	 Â knowledge	 Â  that	 Â  can	 Â  take	 Â a	 Â  lifetime	 Â  to	 Â attain.	 Â 
For	 Â  that	 Â  reason,	 Â  the	 Â  application	 Â  of	 Â  machine	 Â  learning	 Â  algorithms	 Â  in	 Â  author	 Â 
identification	 Â is	 Â a	 Â problem	 Â that	 Â has	 Â garnered	 Â a	 Â lot	 Â of	 Â research	 Â interest.	 Â Our	 Â objective	 Â 
for	 Â  this	 Â  project	 Â  is	 Â  to	 Â  apply	 Â  some	 Â  of	 Â  the	 Â  algorithms	 Â  that	 Â  we	 Â  learned	 Â  in	 Â  class	 Â  on	 Â  the	 Â 
problem	 Â  of	 Â  author	 Â  identification.	 Â  More	 Â  specifically,	 Â  we	 Â  will	 Â  pose	 Â  the	 Â  following	 Â  two	 Â 
problems:	 Â 
â€¢  Select	 Â a	 Â book	 Â and	 Â choose	 Â between	 Â the	 Â actual	 Â author	 Â and	 Â another.	 Â 
â€¢  Select	 Â  a	 Â  book	 Â  and	 Â  identify	 Â  the	 Â  author	 Â  from	 Â  a	 Â  larger	 Â  pool	 Â  (containing	 Â  the	 Â 
actual	 Â author)	 Â 
	 Â Our	 Â  dataset	 Â  consists	 Â  of	 Â  112	 Â  books	 Â  from	 Â  16	 Â  different	 Â  authors.	 Â  The	 Â  books	 Â  were	 Â 
downloaded	 Â 
in	 Â 
.txt	 Â 
format	 Â 
from	 Â 
the	 Â 
Project	 Â  Gutenberg	 Â  website	 Â 
Dataset	 Â 
(www.gutenberg.org).	 Â  While	 Â  the	 Â  books	 Â  and	 Â  the	 Â  authors,	 Â  as	 Â  well	 Â  as	 Â  their	 Â  numbers,	 Â 
were	 Â  selected	 Â  arbitrarily,	 Â  we	 Â  tried	 Â  to	 Â  keep	 Â  most	 Â  authors	 Â  between	 Â  5	 Â  and	 Â  10	 Â  books.	 Â 
Once	 Â  the	 Â books	 Â were	 Â  selected	 Â and	 Â downloaded,	 Â  they	 Â were	 Â  compiled	 Â  into	 Â  forms	 Â  that	 Â 
could	 Â  be	 Â  used	 Â  in	 Â  our	 Â  algorithms.	 Â  Each	 Â  book	 Â was	 Â  compiled	 Â  firstly	 Â  into	 Â  a	 Â  vector	 Â  of	 Â  all	 Â 
the	 Â  words	 Â  appearing	 Â  in	 Â  the	 Â  book	 Â  and	 Â  their	 Â  occurrences.	 Â  Secondly,	 Â  books	 Â  were	 Â 
compiled	 Â into	 Â vectors	 Â of	 Â the	 Â frequency	 Â of	 Â each	 Â word	 Â and	 Â sentence	 Â size.	 Â 
	 Â A	 Â fairly	 Â obvious	 Â measure	 Â of	 Â an	 Â authorâ€™s	 Â style	 Â is	 Â the	 Â words	 Â that	 Â they	 Â use.	 Â As	 Â such,	 Â our	 Â 
first	 Â  idea	 Â  was	 Â  to	 Â  train	 Â  an	 Â  algorithm	 Â  on	 Â  the	 Â  occurrences	 Â  of	 Â  words	 Â  in	 Â  a	 Â  book.	 Â  As	 Â 
Word	 Â Occurrence	 Â Approach	 Â 
expected,	 Â this	 Â led	 Â to	 Â input	 Â vectors	 Â of	 Â very	 Â large	 Â size,	 Â since	 Â every	 Â word	 Â that	 Â appeared	 Â 
at	 Â  least	 Â  once	 Â  in	 Â  any	 Â  of	 Â  the	 Â  books	 Â  in	 Â  our	 Â  dataset	 Â  had	 Â  to	 Â  be	 Â  accounted	 Â  for.	 Â  Naturally,	 Â 

this	 Â  meant	 Â  that	 Â  the	 Â  algorithms	 Â  that	 Â  we	 Â  would	 Â  have	 Â  to	 Â  run	 Â  would	 Â  be	 Â  very	 Â 
computationally	 Â  expensive.	 Â  In	 Â  order	 Â  to	 Â  remedy	 Â  that,	 Â  we	 Â  tried	 Â  using	 Â  a	 Â  stemming	 Â 
algorithm	 Â  that	 Â would	 Â  reduce	 Â  the	 Â  size	 Â  of	 Â  our	 Â  input	 Â  vectors.	 Â This	 Â makes	 Â  sense	 Â  from	 Â  a	 Â 
linguistic	 Â  point	 Â  of	 Â  view	 Â  as	 Â  well;	 Â  our	 Â  first	 Â  approach,	 Â  with	 Â  a	 Â  â€œfullâ€	 Â  dictionary	 Â 
differentiates	 Â between	 Â the	 Â occurrences	 Â of	 Â different	 Â forms	 Â of	 Â the	 Â same	 Â word.	 Â In	 Â other	 Â 
words,	 Â  the	 Â  â€œfullâ€	 Â  dictionary	 Â  will	 Â  consider	 Â  â€œdoâ€	 Â  and	 Â  â€œdoesâ€	 Â  as	 Â  completely	 Â  different	 Â 
words,	 Â  assuming	 Â  that	 Â  each	 Â  one	 Â  influences	 Â  the	 Â  style	 Â  of	 Â  the	 Â  author	 Â  to	 Â  a	 Â  different	 Â 
extent,	 Â  something	 Â  that	 Â  is	 Â  obviously	 Â  not	 Â  true.	 Â  A	 Â  stemming	 Â  algorithm	 Â  would	 Â  lump	 Â 
these	 Â words	 Â  together,	 Â providing	 Â more	 Â  insight	 Â on	 Â  the	 Â style	 Â of	 Â an	 Â author.	 Â After	 Â a	 Â brief	 Â 
online	 Â 
search,	 Â 
the	 Â 
Porter	 Â 
stemming	 Â 
algorithm	 Â 
(http://tartarus.org/martin/PorterStemmer/)	 Â was	 Â used.	 Â Furthermore,	 Â clearly	 Â being	 Â 
in	 Â  a	 Â  supervised	 Â  learning	 Â  setting,	 Â  we	 Â  tried	 Â  NaÃ¯ve	 Â  Bayes	 Â  and	 Â  Logistic	 Â  Regression	 Â  as	 Â  a	 Â 
first	 Â approach.	 Â 
	 Â A	 Â NaÃ¯ve	 Â Bayes	 Â was	 Â implemented	 Â and	 Â tested	 Â for	 Â two	 Â different	 Â problems.	 Â 	 Â 
â€¢  The	 Â  first	 Â  will	 Â  be	 Â  called	 Â  binary	 Â  classification.	 Â  Selecting	 Â  two	 Â  authors	 Â  and	 Â  a	 Â  book	 Â 
NaÃ¯ve	 Â Bayes	 Â 
that	 Â was	 Â written	 Â  by	 Â  one	 Â  of	 Â  the	 Â  two,	 Â we	 Â want	 Â  to	 Â  predict	 Â  the	 Â  actual	 Â writer.	 Â  It	 Â was	 Â 
tested	 Â  for	 Â  a	 Â  few	 Â  author	 Â  couple,	 Â  using	 Â  Leave	 Â  One	 Â  Out	 Â  Crossed	 Â  Classification	 Â 
(LOOCV),	 Â and	 Â each	 Â time	 Â had	 Â 0%	 Â error.	 Â 	 Â 
â€¢  The	 Â  second	 Â will	 Â  be	 Â  called	 Â One	 Â  Versus	 Â  All,	 Â  and	 Â  is	 Â  a	 Â way	 Â  to	 Â  apply	 Â NaÃ¯ve	 Â  Bayes	 Â  to	 Â 
multivariate	 Â  prediction.	 Â When	 Â  a	 Â  prediction	 Â  has	 Â  to	 Â  be	 Â  done,	 Â  the	 Â  algorithm	 Â  is	 Â  first	 Â 
trained	 Â on	 Â all	 Â authors,	 Â labeling	 Â 1	 Â if	 Â the	 Â author	 Â considered	 Â wrote	 Â the	 Â book	 Â and	 Â 0	 Â if	 Â 
not.	 Â When	 Â  testing	 Â  on	 Â  a	 Â  given	 Â  book,	 Â  the	 Â  author	 Â  selected	 Â  is	 Â  the	 Â  one	 Â  that	 Â  gives	 Â  the	 Â 
highest	 Â  probability	 Â  of	 Â  â€œbeing	 Â  written	 Â  by	 Â  the	 Â  considered	 Â  authorâ€.	 Â  This	 Â  approach	 Â 
surprisingly	 Â gave	 Â good	 Â results:	 Â using	 Â LOOCV,	 Â the	 Â error	 Â obtained	 Â was	 Â 18%.	 Â 
	 Â 
	 Â As	 Â  one	 Â  of	 Â  the	 Â  simpler	 Â  supervised	 Â  learning	 Â  algorithms,	 Â  we	 Â  tried	 Â  using	 Â  logistic	 Â 
regression	 Â  for	 Â  our	 Â  problem.	 Â  At	 Â  first,	 Â  we	 Â  used	 Â  logistic	 Â  regression	 Â  for	 Â  binary	 Â 
Logistic	 Â Regression	 Â 
classification.	 Â Using	 Â  LOOCV	 Â  and	 Â  all	 Â possible	 Â permutations	 Â  of	 Â  author	 Â pairs,	 Â  the	 Â  final	 Â 
test	 Â  error	 Â was	 Â  29%.	 Â  Furthermore,	 Â  the	 Â  error	 Â was	 Â  not	 Â  significantly	 Â  affected	 Â when	 Â  the	 Â 
reduced	 Â dictionary	 Â that	 Â was	 Â obtained	 Â from	 Â the	 Â stemming	 Â algorithm	 Â was	 Â used.	 Â Using	 Â 
the	 Â  reduced	 Â  dictionary	 Â  did,	 Â  however,	 Â  greatly	 Â  reduce	 Â  the	 Â  time	 Â  that	 Â  the	 Â  algorithm	 Â 
needed	 Â to	 Â run.	 Â 
In	 Â  trying	 Â  to	 Â  select	 Â  the	 Â  author	 Â  of	 Â  a	 Â  book	 Â  from	 Â  the	 Â  entire	 Â  set	 Â  of	 Â  authors,	 Â  the	 Â  following	 Â 
procedure	 Â was	 Â followed:	 Â 
A	 Â  logistic	 Â  regression	 Â  algorithm	 Â  was	 Â  trained	 Â  for	 Â  each	 Â  author	 Â  on	 Â  the	 Â  entire	 Â  dataset	 Â 
with	 Â  the	 Â  exception	 Â  of	 Â  one	 Â  book.	 Â  The	 Â  label	 Â  0	 Â  was	 Â  the	 Â  case	 Â  when	 Â  the	 Â  book	 Â  did	 Â  not	 Â 
belong	 Â  to	 Â  the	 Â  author	 Â  and	 Â  the	 Â  label	 Â  1	 Â  was	 Â  the	 Â  case	 Â  when	 Â  the	 Â  book	 Â  belonged	 Â  to	 Â  the	 Â 
author.	 Â  The	 Â  quantity	 Â  hÎ¸(x)	 Â was	 Â  calculated	 Â  for	 Â  each	 Â  author	 Â  and	 Â  the	 Â  predicted	 Â  author	 Â 
was	 Â selected	 Â as	 Â the	 Â one	 Â with	 Â the	 Â largest	 Â hÎ¸(x).	 Â Firstly,	 Â the	 Â input	 Â vector	 Â contained	 Â the	 Â 
occurrences	 Â  of	 Â  words.	 Â  Due	 Â  to	 Â  the	 Â  fact	 Â  that	 Â  there	 Â  were	 Â  some	 Â  words	 Â  (such	 Â  as	 Â  â€œaâ€	 Â  or	 Â 

0.35

0.3

0.25

s
e
i
c
n
e
u
q
e
r
F

0.2

0.15

0.1

0.05

0

 
0

â€œtheâ€)	 Â had	 Â a	 Â much	 Â  larger	 Â number	 Â of	 Â occurrences	 Â  than	 Â others	 Â and	 Â  that	 Â  the	 Â zero-Â­â€label	 Â 
training	 Â examples	 Â were	 Â a	 Â lot	 Â more,	 Â Î¸â€™x	 Â was	 Â a	 Â very	 Â large	 Â number	 Â and	 Â the	 Â exponential	 Â 
of	 Â  it	 Â  tended	 Â  to	 Â  infinity	 Â  and	 Â  the	 Â  algorithm	 Â  failed.	 Â  Then,	 Â  the	 Â  occurrences	 Â  were	 Â 
normalized	 Â  by	 Â  the	 Â  number	 Â  of	 Â  words	 Â  in	 Â  the	 Â  book	 Â  (essentially	 Â  representing	 Â 
frequencies).	 Â Even	 Â in	 Â that	 Â case,	 Â this	 Â algorithm	 Â had	 Â 100%	 Â test	 Â error.	 Â 
	 Â 
	 Â 
Word/Sentence	 Â Length	 Â Approach	 Â 

Sentence length frequencies in Emma (Austen)
Compiled frequencies
Logâˆ’normal interpolation

 

Word length frequencies in Emma (Austen)
Compiled frequencies
Logâˆ’normal interpolated frequencies

 

0.07

0.06

0.05

y
c
n
e
u
q
e
r
F

0.04

0.03

Figure 	 Â 1	 Â 
	 Â 	 Â 	 Â Our	 Â  language	 Â  is	 Â made	 Â of	 Â words	 Â  that	 Â are	 Â 
5
10
15
Word length (in caracters)
characters	 Â combinations.	 Â Under	 Â certain	 Â 
grammatical	 Â  rules,	 Â  words	 Â  are	 Â  also	 Â 
combined	 Â to	 Â make	 Â sentences.	 Â When	 Â we	 Â 
write,	 Â  we	 Â  have	 Â  a	 Â  personal	 Â  way	 Â  to	 Â 
combine	 Â words	 Â  to	 Â make	 Â  sentences	 Â  that	 Â 
will	 Â  be	 Â  long	 Â  or	 Â  short	 Â  depending	 Â  on	 Â  our	 Â 
style.	 Â  For	 Â  instance,	 Â  Proustâ€™s	 Â  style	 Â  is	 Â 
recognizable	 Â  because	 Â  of	 Â  his	 Â  usage	 Â  of	 Â 
very	 Â  long	 Â  sentences.	 Â  To	 Â  some	 Â  extend,	 Â 
wordâ€™s	 Â  length	 Â  usage	 Â  might	 Â  also	 Â  be	 Â  an	 Â 
indication	 Â  of	 Â  some	 Â  writerâ€™s	 Â  literary	 Â 
style.	 Â 	 Â 
Instead	 Â  of	 Â  building	 Â  a	 Â  vocabulary	 Â  for	 Â 
our	 Â  dataset	 Â  and	 Â  then	 Â  count	 Â  for	 Â  each	 Â 

40

80

Figure 	 Â 2	 Â 
60
Sentence length
Lognormal distribution on phrase length

100

120

 

Austen
Dickens
Dostoievsky
Doyle
Dumas
Goethe
London

20

0.02

0.01

0

 
0

2

1.8

1.6

1.4

1.2

1

0.8

0.6

n
o
i
t
a
i
v
e
D
 
d
r
a
d
n
a
t
S

 
0.4
1.8

2

2.2

Figure 	 Â 3	 Â 
2.4
2.6
Mean

2.8

3

3.2

book	 Â  the	 Â wordsâ€™	 Â occurrences,	 Â  frequencies	 Â  for	 Â  each	 Â word	 Â  and	 Â  sentence	 Â  lengths	 Â were	 Â 
computed	 Â for	 Â each	 Â book,	 Â and	 Â each	 Â book	 Â was	 Â then	 Â represented	 Â by	 Â two	 Â vectors:	 Â 	 Â 
â€¢  One	 Â containing	 Â the	 Â word	 Â length	 Â frequencies,	 Â with	 Â the	 Â ith	 Â component	 Â being	 Â the	 Â 
frequency	 Â of	 Â the	 Â words	 Â of	 Â i	 Â characters	 Â 
â€¢  The	 Â  other	 Â  containing	 Â  the	 Â  sentences	 Â  length	 Â  frequencies,	 Â  with	 Â  the	 Â  ith	 Â 
component	 Â being	 Â the	 Â frequency	 Â of	 Â the	 Â sentences	 Â made	 Â of	 Â i	 Â words	 Â 
This	 Â  approach	 Â  has	 Â  a	 Â  double	 Â  advantage:	 Â  first	 Â  it	 Â  reduces	 Â  considerably	 Â  the	 Â  size	 Â  of	 Â  the	 Â 
data.	 Â  Word	 Â  lengths	 Â  frequencies	 Â  are	 Â  null	 Â  for	 Â  sizes	 Â  higher	 Â  than	 Â  20	 Â  characters,	 Â  and	 Â 
sentence	 Â lengths	 Â frequencies	 Â are	 Â usually	 Â null	 Â for	 Â length	 Â of	 Â more	 Â than	 Â 200	 Â words.	 Â But	 Â 
it	 Â  also	 Â  makes	 Â  it	 Â  possible	 Â  to	 Â  actually	 Â  visualize	 Â  the	 Â  data.	 Â  Figure	 Â  1	 Â  and	 Â  Figure	 Â  2	 Â  show	 Â 
the	 Â  frequency	 Â  distributions	 Â  for	 Â  respectively	 Â word	 Â  lengths	 Â  and	 Â  sentences	 Â  lengths	 Â  for	 Â 
Austenâ€™s	 Â novel	 Â Emma.	 Â 
But	 Â  we	 Â  can	 Â  go	 Â  further	 Â  and	 Â  notice	 Â  that	 Â  both	 Â  distributions	 Â  look	 Â  like	 Â  they	 Â  are	 Â  log-Â­â€
normally	 Â  distributed.	 Â  And	 Â  since	 Â  our	 Â  vectors	 Â  are	 Â  discrete,	 Â  it	 Â  is	 Â  very	 Â  easy	 Â  to	 Â  get	 Â  the	 Â 
mean	 Â  and	 Â  the	 Â  variance,	 Â  the	 Â  only	 Â  two	 Â  parameters	 Â  that	 Â  fully	 Â  describe	 Â  a	 Â  given	 Â 
distribution.	 Â  If	 Â  we	 Â  note	 Â  X(i)	 Â  the	 Â  frequency	 Â  of	 Â  the	 Â  word	 Â  or	 Â  sentence	 Â  of	 Â  length	 Â  i,	 Â  we	 Â 
have:	 Â 
ğœ‡ =
log ğ‘– ğ‘‹ ğ‘–  Â ğ‘ğ‘›ğ‘‘  Â ğœ =  Â 
log ğ‘– ! ğ‘‹ ğ‘– âˆ’ ğœ‡! 	 Â 
	 Â This	 Â allows	 Â us	 Â now	 Â to	 Â represent	 Â a	 Â book	 Â in	 Â 4	 Â dimensions:	 Â 	 Â 
	 Â 
[book]	 Â =	 Â [Âµword,	 Â Ïƒword,	 Â Âµsentence,	 Â Ïƒsentence]	 Â 
Figure	 Â  3	 Â  shows	 Â  a	 Â  reduced	 Â  form	 Â  of	 Â  the	 Â  data	 Â  in	 Â  2	 Â  dimensions	 Â  for	 Â  a	 Â  reduced	 Â  set	 Â  of	 Â 
authors	 Â  (only	 Â  7	 Â  of	 Â  them),	 Â  using	 Â  the	 Â  mean	 Â  and	 Â  variance	 Â  for	 Â  phrase	 Â  lengths.	 Â  This	 Â 
shows	 Â that	 Â using	 Â this	 Â criteria,	 Â some	 Â author	 Â can	 Â be	 Â distinguished,	 Â Austen	 Â and	 Â London	 Â 
for	 Â  instance,	 Â  whereas	 Â  some	 Â  couple	 Â  of	 Â  authors	 Â  cannot	 Â  be	 Â  separated	 Â  using	 Â  linear	 Â 
regression	 Â or	 Â some	 Â clustering	 Â algorithm,	 Â London	 Â and	 Â Doyle	 Â for	 Â instance.	 Â 
NaÃ¯ve-Â­â€Bayes	 Â  and	 Â  logistic	 Â  regression	 Â  were	 Â  tried	 Â  on	 Â  both	 Â  frequency	 Â  vectors	 Â  and	 Â 
interpolated	 Â  parameters	 Â  vectors	 Â  [Âµword,	 Â 
Ïƒword,	 Â  Âµsentence,	 Â  Ïƒsentence].	 Â  For	 Â  each	 Â  existing	 Â 
author	 Â  couple,	 Â  LOOCV	 Â  was	 Â  used	 Â  and	 Â  the	 Â 
error	 Â  incremented	 Â when	 Â  the	 Â  algorithm	 Â was	 Â 
making	 Â an	 Â incorrect	 Â prediction.	 Â 	 Â 
1
For	 Â NaÃ¯ve	 Â Bayes,	 Â surprisingly,	 Â the	 Â reduced	 Â 
0.9
4-Â­â€component	 Â vector	 Â test	 Â gave	 Â a	 Â lower	 Â 
error:	 Â 46%	 Â compared	 Â to	 Â 47%	 Â for	 Â the	 Â 
frequency	 Â vectors.	 Â Also,	 Â the	 Â error	 Â for	 Â each	 Â 
couple	 Â was	 Â calculated	 Â and	 Â was	 Â highly	 Â 
fluctuating	 Â (from	 Â 25%	 Â to	 Â 100%)	 Â depending	 Â 
on	 Â the	 Â couple	 Â considered.	 Â That	 Â confirms	 Â the	 Â 
visual	 Â intuition	 Â we	 Â got	 Â looking	 Â at	 Â Figure	 Â 3.	 Â 	 Â 
0.3
For	 Â logistic	 Â regression,	 Â the	 Â one	 Â versus	 Â all	 Â 
approach	 Â gives	 Â surprisingly	 Â better	 Â results	 Â 
0.2
than	 Â the	 Â word	 Â occurrence	 Â approach.	 Â The	 Â binary	 Â classification	 Â 
0.1
0
0
10
20
30

Figure	 Â 4	 Â 
50
60
Author Pair ID

r
o
r
r
E
 
t
s
e
T

0.5

0.8

0.7

0.6

0.4

40

70

80

90

100

performance	 Â strongly	 Â depends	 Â on	 Â the	 Â pair	 Â of	 Â author	 Â considered	 Â though,	 Â as	 Â it	 Â is	 Â 
shown	 Â on	 Â Figure	 Â 4.	 Â 
	 Â 
	 Â The	 Â errors	 Â for	 Â each	 Â algorithm	 Â and	 Â set	 Â of	 Â data	 Â used	 Â are	 Â displayed	 Â below:	 Â 
	 Â 	 Â 
Conclusion	 Â and	 Â Suggestions	 Â for	 Â Further	 Â Work	 Â 
NaÃ¯ve-Â­â€Bayes	 Â 
NaÃ¯ve-Â­â€
Logistic	 Â 
Logistic	 Â regression	 Â 
Data	 Â used	 Â 
â€œmultivariateâ€	 Â 
regression	 Â 
Bayes	 Â 
â€œmultivariateâ€	 Â 
18%	 Â 
0%	 Â 
100%	 Â 
29%	 Â 
Word	 Â occurrence	 Â 
Word	 Â occurrence	 Â with	 Â 
29%	 Â 
100%	 Â 
0%	 Â 
18%	 Â 
stemming	 Â 
Word/sentence	 Â 
51%	 Â 
89%	 Â 
25-Â­â€100%	 Â 
100%	 Â 
47%	 Â ave.	 Â 
distribution	 Â 
87%	 Â 
89%	 Â 
51%	 Â 
[book]	 Â =	 Â [Âµword,	 Â Ïƒword,	 Â 
25-Â­â€100%	 Â 
46%	 Â ave.	 Â 
Âµsentence,	 Â Ïƒsentence]	 Â 
	 Â 
	 Â NaÃ¯ve	 Â Bayes	 Â for	 Â word	 Â occurrences	 Â works	 Â very	 Â well	 Â to	 Â recognize	 Â one	 Â author	 Â in	 Â a	 Â pool	 Â 
of	 Â  two.	 Â  It	 Â  works	 Â  well	 Â  also	 Â  when	 Â  trying	 Â  to	 Â  recognize	 Â  an	 Â  author	 Â  in	 Â  a	 Â  larger	 Â  pool.	 Â 
Running	 Â  the	 Â  algorithm	 Â  takes	 Â  a	 Â  lot	 Â  of	 Â  time	 Â  though	 Â  since	 Â  matrices	 Â  of	 Â  size	 Â  of	 Â  about	 Â 
40,000	 Â  components	 Â  are	 Â  manipulated.	 Â  Also,	 Â  the	 Â  other	 Â  attempt	 Â  to	 Â  make	 Â  shorter	 Â 
vectors	 Â  showed	 Â  that	 Â  phrase	 Â  lengths	 Â  and	 Â  word	 Â  lengths	 Â  frequencies	 Â  were	 Â  of	 Â  some	 Â 
significance	 Â to	 Â characterize	 Â one	 Â authorâ€™s	 Â style.	 Â However,	 Â the	 Â error	 Â found	 Â is	 Â very	 Â high	 Â 
on	 Â average.	 Â Some	 Â authors	 Â may	 Â be	 Â distinguished	 Â with	 Â these	 Â parameters	 Â but	 Â it	 Â cannot	 Â 
be	 Â  applied	 Â  universally	 Â  to	 Â  all	 Â  authors.	 Â  In	 Â  particular,	 Â  the	 Â  100%	 Â  error	 Â  for	 Â  the	 Â  one	 Â 
versus	 Â  all	 Â  NaÃ¯ve	 Â  Bayes	 Â  can	 Â  be	 Â  explained	 Â  by	 Â  the	 Â  way	 Â  the	 Â  algorithm	 Â  works,	 Â  since	 Â  it	 Â 
choses	 Â between	 Â  the	 Â 13	 Â authors.	 Â Also,	 Â surprisingly,	 Â one-Â­â€versus-Â­â€all	 Â  logistic	 Â regression	 Â 
performs	 Â better	 Â using	 Â  the	 Â  frequencies	 Â or	 Â  interpolation	 Â parameters	 Â  than	 Â when	 Â using	 Â 
word	 Â occurrences.	 Â 
	 Â Since	 Â  input	 Â  data	 Â  was	 Â  entire	 Â  books	 Â  here,	 Â  it	 Â  appeared	 Â  early	 Â  in	 Â  our	 Â  work	 Â  that	 Â  the	 Â 
information	 Â  that	 Â  we	 Â  chose	 Â  to	 Â  extract	 Â  from	 Â  them	 Â  was	 Â  of	 Â  high	 Â  importance	 Â  for	 Â  the	 Â 
performance	 Â  of	 Â  our	 Â  algorithms.	 Â We	 Â  chose	 Â  to	 Â work	 Â with	 Â  relatively	 Â  simple	 Â  algorithm,	 Â 
logistic	 Â regression	 Â and	 Â NaÃ¯ve	 Â Bayes,	 Â  to	 Â  focus	 Â on	 Â  the	 Â data	 Â extracted	 Â and	 Â  its	 Â relevance	 Â 
to	 Â  the	 Â  style	 Â  recognition	 Â  problem.	 Â  Finding	 Â  the	 Â  relevant	 Â  data	 Â  is	 Â  key	 Â  here,	 Â  and	 Â  should	 Â 
we	 Â have	 Â more	 Â time,	 Â we	 Â would	 Â spend	 Â it	 Â on	 Â this	 Â key	 Â issue.	 Â 
	 Â 	 Â 

