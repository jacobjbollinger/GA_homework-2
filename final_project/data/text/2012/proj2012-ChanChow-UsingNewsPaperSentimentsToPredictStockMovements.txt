CS229	 Â Project	 Â Report	 Â 
Using	 Â Newspaper	 Â Sentiments	 Â to	 Â Predict	 Â Stock	 Â Movements	 Â 
Hao	 Â Yee	 Â Chan	 Â 	 Â 	 Â 	 Â 	 Â 	 Â Anthony	 Â Chow	 Â 
haoyeec@stanford.edu	 Â 	 Â 	 Â ac1408@stanford.edu	 Â 
	 Â 

	 Â 

Problem	 Â Statement	 Â 
It	 Â is	 Â often	 Â said	 Â that	 Â stock	 Â prices	 Â are	 Â determined	 Â by	 Â market	 Â sentiments.	 Â Also,	 Â these	 Â stock	 Â prices	 Â are	 Â an	 Â 
instant	 Â reflection	 Â of	 Â the	 Â current	 Â market	 Â sentiments.	 Â Despite	 Â this,	 Â investors	 Â often	 Â use	 Â current	 Â news	 Â to	 Â 
inform	 Â their	 Â next	 Â investment	 Â decision.	 Â The	 Â problem	 Â is	 Â then	 Â that	 Â it	 Â is	 Â almost	 Â impossible	 Â to	 Â read	 Â through	 Â all	 Â 
the	 Â news	 Â available	 Â online.	 Â Even	 Â with	 Â a	 Â wealth	 Â of	 Â readings,	 Â market	 Â sentiments	 Â are	 Â difficult	 Â to	 Â be	 Â quantified	 Â 
and	 Â understood.	 Â 	 Â 
	 Â 
This	 Â project	 Â looks	 Â at	 Â news	 Â from	 Â Reuters	 Â Technology	 Â to	 Â be	 Â used	 Â as	 Â sources	 Â of	 Â data	 Â to	 Â generate	 Â a	 Â model	 Â to	 Â 
capture	 Â market	 Â sentiments.	 Â This	 Â model	 Â will	 Â be	 Â used	 Â to	 Â try	 Â to	 Â predict	 Â the	 Â movements	 Â of	 Â the	 Â NASDAQ	 Â 
Composite	 Â in	 Â the	 Â immediate	 Â future.	 Â 
	 Â 
Dataset	 Â 
We	 Â scrapped	 Â data	 Â Reuters	 Â to	 Â create	 Â a	 Â model	 Â for	 Â market	 Â sentiments.	 Â We	 Â will	 Â be	 Â using	 Â yahoo	 Â share	 Â prices	 Â 
to	 Â construct	 Â our	 Â classifiers,	 Â which	 Â will	 Â be	 Â the	 Â NASDAQ	 Â Composite,	 Â which	 Â is	 Â highly	 Â followed	 Â in	 Â the	 Â US	 Â as	 Â an	 Â 
indicator	 Â of	 Â the	 Â performance	 Â of	 Â stocks	 Â of	 Â technology	 Â companies	 Â and	 Â growth	 Â companies.	 Â (see	 Â appendix	 Â for	 Â 
screenshot	 Â of	 Â Reuters	 Â Technology)	 Â We	 Â created	 Â 2	 Â separate	 Â datasets,	 Â one	 Â with	 Â 1	 Â year	 Â of	 Â data	 Â (260	 Â days	 Â of	 Â 
trading)	 Â and	 Â the	 Â second	 Â one	 Â with	 Â 3	 Â years	 Â of	 Â data	 Â (690	 Â days	 Â of	 Â trading).	 Â Note	 Â that	 Â there	 Â are	 Â less	 Â days	 Â of	 Â 
trading	 Â than	 Â there	 Â are	 Â in	 Â a	 Â year	 Â due	 Â to	 Â market	 Â closing	 Â during	 Â weekends	 Â as	 Â well	 Â as	 Â during	 Â public	 Â holidays.	 Â 
	 Â 
Problem	 Â Formulation	 Â 
We	 Â split	 Â the	 Â news	 Â data	 Â from	 Â Reuters	 Â Technology	 Â into	 Â headline	 Â and	 Â body	 Â feature	 Â sets.	 Â We	 Â aim	 Â to	 Â predict,	 Â 
ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘› =  Â  1 ğ‘–ğ‘“  Â ğ‘ ğ‘¡ğ‘œğ‘ğ‘˜  Â ğ‘šğ‘œğ‘£ğ‘’ğ‘   Â ğ‘¢ğ‘ğ‘¤ğ‘ğ‘Ÿğ‘‘ğ‘ 
given	 Â todayâ€™s	 Â set	 Â of	 Â headline	 Â and	 Â body	 Â features,	 Â if	 Â the	 Â closing	 Â price	 Â of	 Â tomorrowâ€™s	 Â stock	 Â will	 Â be	 Â higher	 Â or	 Â 
âˆ’1 Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 	 Â 
lower	 Â than	 Â todayâ€™s,	 Â using	 Â data	 Â from	 Â yahoo	 Â finance.	 Â 
	 Â 
	 Â 
We	 Â did	 Â some	 Â preprocessing	 Â to	 Â build	 Â our	 Â dataset.	 Â From	 Â the	 Â news,	 Â we	 Â first	 Â split	 Â them	 Â into	 Â 2	 Â sets:	 Â Headline	 Â 
and	 Â body.	 Â For	 Â each	 Â set,	 Â we	 Â had	 Â stop	 Â words	 Â removed,	 Â the	 Â words	 Â lemmatized	 Â and	 Â selected	 Â 500	 Â words	 Â using	 Â 
ğ‘€ğ¼=  Â 
Î§ ! =(!!!!!!"!!!"!!!! ) Â  !!!!!!!!!"!!" !
the	 Â following	 Â heuristics:	 Â 
ğ‘ ğ‘¥ , ğ‘¦ ğ‘™ğ‘œğ‘” ğ‘(ğ‘¥ , ğ‘¦)
	 Â 
ğ‘ ğ‘¥ ğ‘(ğ‘¦)
Mutual	 Â Information	 Â 
Chi-Â­â€squared	 Â 
where	 Â ğ‘!" =Number	 Â of	 Â news	 Â 
!!!!!!" (!!!!!!" )(!!"!!!! )(!!"!!!! )	 Â 	 Â 
articles	 Â with	 Â word	 Â ğ‘¡ 	 Â and	 Â class	 Â ğ‘ 	 Â 
!âˆˆ!
!âˆˆ!
Table	 Â 1:	 Â Feature	 Â selection	 Â techniques	 Â used	 Â 
Our	 Â Methodology	 Â 	 Â 
We	 Â used	 Â a	 Â mixture	 Â of	 Â supervised	 Â and	 Â unsupervised	 Â machine	 Â learning	 Â techniques	 Â to	 Â figure	 Â out	 Â our	 Â data.	 Â 
Under	 Â the	 Â unsupervised	 Â techniques,	 Â we	 Â used	 Â factor	 Â analysis	 Â with	 Â EM	 Â to	 Â look	 Â at	 Â some	 Â of	 Â the	 Â key	 Â 
dimensions	 Â that	 Â described	 Â the	 Â data.	 Â We	 Â also	 Â compared	 Â the	 Â performance	 Â of	 Â the	 Â various	 Â supervised	 Â 
learning	 Â algorithms	 Â on	 Â the	 Â dataset.	 Â A	 Â summary	 Â of	 Â the	 Â supervised	 Â learning	 Â algorithms	 Â implemented	 Â in	 Â this	 Â 
	 Â 
1	 Â 
paper	 Â is	 Â summarized	 Â in	 Â the	 Â table	 Â below.	 Â 

Bag-Â­â€of-Â­â€Words	 Â 
Most	 Â frequent	 Â words	 Â 

	 Â 

Multinomial	 Â NaÃ¯ve	 Â Bayes	 Â 
Gaussian	 Â Discriminant	 Â Analysis	 Â 
Support	 Â Vector	 Â Machines	 Â 
Headline	 Â features	 Â (1	 Â year)	 Â 
-Â­â€	 Â 
Headline	 Â features	 Â (1	 Â year)	 Â 
Body	 Â features	 Â (1	 Â year)	 Â 
Body	 Â features	 Â (1	 Â year)	 Â 
Body	 Â features	 Â (1	 Â year)	 Â 
Headline	 Â features	 Â (3	 Â years)	 Â 
-Â­â€	 Â 
Headline	 Â features	 Â (3	 Â years)	 Â 
Body	 Â features	 Â (3	 Â years)	 Â 
Body	 Â features	 Â (3	 Â years)	 Â 
Body	 Â features	 Â (3	 Â years)	 Â 
Table	 Â 2:	 Â Table	 Â of	 Â summary	 Â of	 Â supervised	 Â learning	 Â algorithms	 Â implemented	 Â 

	 Â 
Our	 Â Results	 Â 	 Â 
Factor	 Â Analysis	 Â of	 Â Data	 Â 
We	 Â implemented	 Â factor	 Â analysis	 Â on	 Â the	 Â body	 Â feature	 Â sets.	 Â We	 Â present	 Â the	 Â results	 Â obtained	 Â from	 Â the	 Â body	 Â 
feature	 Â sets	 Â (1	 Â year	 Â and	 Â 3	 Â years)	 Â generated	 Â from	 Â frequent	 Â words.	 Â We	 Â find	 Â that	 Â there	 Â seem	 Â to	 Â exist	 Â 3	 Â main	 Â 
dimensions	 Â in	 Â the	 Â data	 Â â€“	 Â Finance,	 Â Facebook	 Â and	 Â Apple	 Â that	 Â characterized	 Â the	 Â news	 Â from	 Â last	 Â year.	 Â For	 Â the	 Â 
3	 Â years	 Â data,	 Â it	 Â seemed	 Â to	 Â be	 Â â€“	 Â Finance,	 Â Apple	 Â and	 Â everyone	 Â else.	 Â Perhaps	 Â the	 Â Facebook	 Â IPO	 Â in	 Â this	 Â past	 Â 
year	 Â generated	 Â enough	 Â coverage	 Â to	 Â create	 Â this	 Â unique	 Â dimension	 Â in	 Â the	 Â data.	 Â Armed	 Â with	 Â the	 Â idea	 Â that	 Â 
there	 Â were	 Â special	 Â dimensions	 Â in	 Â the	 Â data,	 Â that	 Â it	 Â was	 Â not	 Â as	 Â random	 Â as	 Â we	 Â thought,	 Â we	 Â went	 Â to	 Â perform	 Â 
supervised	 Â learning	 Â with	 Â more	 Â confidence.	 Â 

	 Â 
	 Â 
Table	 Â 3:	 Â Dimensions	 Â obtained	 Â using	 Â factor	 Â analysis	 Â on	 Â body	 Â feature	 Â set	 Â of	 Â 1-Â­â€year	 Â (left)	 Â and	 Â 3-Â­â€years	 Â (right)	 Â 
2	 Â 
of	 Â data	 Â respectively	 Â 

	 Â 

Supervised	 Â Learning	 Â 1	 Â â€“	 Â Multinomial	 Â NaÃ¯ve	 Â Bayes	 Â 
	 Â 
We	 Â started	 Â the	 Â supervised	 Â learning	 Â with	 Â Multinomial	 Â NaÃ¯ve	 Â Bayes.	 Â The	 Â datasets	 Â were	 Â split	 Â into	 Â 2/3	 Â training	 Â 
	 Â  ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘†ğ‘’ğ‘¡ ğ‘¡ =  Â ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘†ğ‘’ğ‘¡ ğ‘¡ +  Â  ğ›¿!2 ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘†ğ‘’ğ‘¡ ğ‘¡ âˆ’ 1 +  Â  ğ›¿!3 ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘†ğ‘’ğ‘¡ ğ‘¡ âˆ’ 2 +  Â  ğ›¿!4 ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ğ‘†ğ‘’ğ‘¡(ğ‘¡ âˆ’ 3)	 Â 
and	 Â 1/3	 Â testing	 Â sets.	 Â Observing	 Â that	 Â there	 Â exist	 Â imperfections	 Â in	 Â the	 Â market,	 Â we	 Â created	 Â a	 Â cumulative	 Â 
feature	 Â set	 Â that	 Â takes	 Â into	 Â account	 Â information	 Â from	 Â past	 Â news	 Â in	 Â the	 Â following	 Â fashion:	 Â 
ğ›¿! = 1 Â ğ‘–ğ‘“  Â ğ‘¡ = 1,2,3 Â , ğ‘ğ‘›ğ‘‘  Â 0 Â ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’  Â 	 Â 
ğ›¿! = 1 Â ğ‘–ğ‘“  Â ğ‘¡ = 2,3 Â , ğ‘ğ‘›ğ‘‘  Â 0 Â ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’  Â 
ğ›¿! = 1 Â ğ‘–ğ‘“  Â ğ‘¡ = 3, ğ‘ğ‘›ğ‘‘  Â 0 Â ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ 	 Â 
 Â ğ‘¡ = 0,1,2,3 Â ğ‘‘ğ‘ğ‘¦ğ‘  	 Â 

	 Â 
Figure	 Â 1:	 Â An	 Â example	 Â of	 Â test	 Â statistics	 Â trained	 Â using	 Â chi-Â­â€squared	 Â feature	 Â selection.	 Â 

	 Â 
Remarks:	 Â 1)	 Â Best	 Â performance	 Â comes	 Â from	 Â feature	 Â sets	 Â that	 Â contain	 Â information	 Â only	 Â from	 Â current	 Â day	 Â or	 Â 
with	 Â one	 Â extra	 Â day	 Â behind.	 Â 	 Â 2)	 Â Higher	 Â dimension	 Â feature	 Â set	 Â needed	 Â for	 Â better	 Â performance	 Â for	 Â more	 Â 
number	 Â of	 Â days	 Â incorporated.	 Â (See	 Â figure,	 Â 120	 Â features	 Â for	 Â 0	 Â days,	 Â 240	 Â features	 Â for	 Â 1	 Â day,	 Â 350	 Â features	 Â for	 Â 
2	 Â days)	 Â 
	 Â 
It	 Â is	 Â interested	 Â that	 Â the	 Â chi-Â­â€squared	 Â and	 Â mutual	 Â information	 Â feature	 Â selection	 Â did	 Â not	 Â perform	 Â as	 Â well	 Â as	 Â 
feature	 Â selection	 Â using	 Â frequent	 Â words.	 Â From	 Â our	 Â experiments,	 Â we	 Â find	 Â that	 Â the	 Â best	 Â performance	 Â came	 Â 
from	 Â feature	 Â set	 Â generated	 Â from	 Â 1	 Â year	 Â of	 Â data	 Â with	 Â frequent	 Â word	 Â feature	 Â selection.	 Â The	 Â test	 Â results	 Â are	 Â 
summarized	 Â in	 Â the	 Â two	 Â tables	 Â below.	 Â 
	 Â 

	 Â 

3	 Â 
	 Â 

Table	 Â 4:	 Â Summary	 Â of	 Â statistics	 Â of	 Â MNB	 Â using	 Â 1	 Â year	 Â of	 Â data	 Â 

Table	 Â 5:	 Â Summary	 Â of	 Â statistics	 Â of	 Â MNB	 Â using	 Â 3	 Â years	 Â of	 Â data	 Â 

	 Â 
Supervised	 Â Learning	 Â 2	 Â â€“	 Â Gaussian	 Â Discriminant	 Â Analysis	 Â 
We	 Â then	 Â tried	 Â using	 Â GDA	 Â to	 Â compare	 Â performance.	 Â 	 Â 

	 Â 

	 Â 
Figure	 Â 2:	 Â An	 Â example	 Â of	 Â test	 Â statistics	 Â trained	 Â using	 Â mutual	 Â information	 Â feature	 Â selection.	 Â 
	 Â 
Remarks:	 Â 1)	 Â The	 Â covariance	 Â matrix	 Â rapidly	 Â becomes	 Â singular	 Â at	 Â higher	 Â feature	 Â spaces	 Â due	 Â to	 Â insufficient	 Â 
training	 Â data.	 Â This	 Â occurs	 Â when	 Â the	 Â number	 Â of	 Â features	 Â is	 Â approximately	 Â equal	 Â to	 Â the	 Â number	 Â of	 Â training	 Â 
data.	 Â 2)	 Â Also,	 Â we	 Â observe	 Â that	 Â the	 Â training	 Â data	 Â gets	 Â fitted	 Â very	 Â well	 Â with	 Â increasing	 Â number	 Â of	 Â features,	 Â 
perhaps	 Â leading	 Â to	 Â over-Â­â€fitting.	 Â 3)	 Â We	 Â are	 Â able	 Â to	 Â obtain	 Â good	 Â accuracy	 Â (60%)	 Â on	 Â the	 Â test	 Â data	 Â set	 Â with	 Â low	 Â 
number	 Â of	 Â features.	 Â Thus	 Â when	 Â it	 Â is	 Â expensive	 Â to	 Â collect	 Â features,	 Â the	 Â GDA	 Â presents	 Â itself	 Â as	 Â a	 Â good	 Â 
alternative	 Â to	 Â MNB.	 Â 	 Â 
	 Â 
The	 Â tables	 Â below	 Â summarize	 Â our	 Â findings	 Â with	 Â GDA.	 Â We	 Â see	 Â that	 Â in	 Â general	 Â the	 Â best	 Â performance	 Â comes	 Â 
from	 Â number	 Â of	 Â features	 Â that	 Â are	 Â significantly	 Â lower	 Â than	 Â that	 Â required	 Â by	 Â the	 Â MNB.	 Â 

	 Â 

Table	 Â 6:	 Â Summary	 Â of	 Â statistics	 Â of	 Â GDA	 Â using	 Â 1	 Â year	 Â of	 Â data	 Â 

	 Â 

4	 Â 

Table	 Â 7:	 Â Summary	 Â of	 Â statistics	 Â of	 Â GDA	 Â using	 Â 3	 Â years	 Â of	 Â data	 Â 

	 Â 
Supervised	 Â Learning	 Â 3	 Â â€“	 Â Support	 Â Vector	 Â Machines	 Â 
Unlike	 Â the	 Â MNB,	 Â there	 Â does	 Â not	 Â seem	 Â to	 Â be	 Â a	 Â clear	 Â trend	 Â in	 Â the	 Â results	 Â of	 Â the	 Â SVM.	 Â From	 Â the	 Â tables	 Â below,	 Â 
we	 Â observe	 Â that	 Â we	 Â get	 Â the	 Â best	 Â results	 Â from	 Â the	 Â SVM	 Â using	 Â the	 Â mutual	 Â information	 Â feature	 Â selection.	 Â All	 Â 
the	 Â results	 Â below	 Â were	 Â calculated	 Â using	 Â linear	 Â kernel.	 Â 	 Â 

	 Â 

Table	 Â 8:	 Â Summary	 Â of	 Â statistics	 Â of	 Â SVM	 Â using	 Â 1	 Â year	 Â of	 Â data	 Â 

	 Â 

	 Â 

Table	 Â 9:	 Â Summary	 Â of	 Â statistics	 Â of	 Â SVM	 Â using	 Â 3	 Â years	 Â of	 Â data	 Â 

	 Â 
Summary	 Â and	 Â Future	 Â Work	 Â 
We	 Â compared	 Â the	 Â performance	 Â of	 Â 3	 Â supervised	 Â classifiers	 Â on	 Â the	 Â Reuters	 Â Technology	 Â news	 Â sectionâ€™s	 Â ability	 Â 
to	 Â predict	 Â the	 Â stock	 Â movements	 Â of	 Â the	 Â NASDAQ	 Â composite.	 Â All	 Â 3	 Â were	 Â able	 Â to	 Â perform	 Â better	 Â than	 Â 
random,	 Â with	 Â SVM	 Â and	 Â NMB	 Â being	 Â able	 Â to	 Â perform	 Â better	 Â than	 Â 65%	 Â accuracy	 Â under	 Â certain	 Â conditions	 Â of	 Â 
feature	 Â selections,	 Â number	 Â of	 Â features	 Â and	 Â number	 Â of	 Â days	 Â of	 Â information	 Â included.	 Â Also,	 Â with	 Â the	 Â 
dimensions	 Â learnt	 Â from	 Â the	 Â factor	 Â analysis,	 Â we	 Â can	 Â show	 Â convincingly	 Â that	 Â our	 Â learning	 Â algorithms	 Â did	 Â pick	 Â 
up	 Â hidden	 Â trends	 Â in	 Â the	 Â data	 Â to	 Â aid	 Â in	 Â prediction.	 Â This	 Â showed	 Â that	 Â there	 Â is	 Â ability	 Â of	 Â news	 Â sentiments	 Â to	 Â 
predict	 Â stock	 Â market	 Â movements	 Â in	 Â the	 Â imperfect	 Â market	 Â conditions	 Â we	 Â live	 Â in	 Â today.	 Â 
	 Â 
The	 Â next	 Â step	 Â would	 Â be	 Â to	 Â build	 Â a	 Â stronger	 Â classifier	 Â based	 Â on	 Â the	 Â 3	 Â weak	 Â classifiers	 Â we	 Â presented	 Â in	 Â this	 Â 
paper.	 Â Also,	 Â other	 Â classification	 Â techniques	 Â like	 Â random	 Â forests	 Â could	 Â be	 Â attempted	 Â as	 Â well.	 Â More	 Â 
interestingly,	 Â we	 Â could	 Â incorporate	 Â news	 Â from	 Â other	 Â sections,	 Â to	 Â see	 Â which	 Â sections	 Â provide	 Â best	 Â 
prediction	 Â capabilities	 Â for	 Â tomorrowâ€™s	 Â stock	 Â price	 Â movements.	 Â 
	 Â 
Bibliography	 Â 
1.	 Â Andrew	 Â Ng,	 Â CS	 Â 229	 Â Machine	 Â Learning,	 Â Stanford	 Â University	 Â 2012	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 
	 Â 

5	 Â 

Appendix	 Â 
Screenshots	 Â of	 Â Reuters	 Â Technology	 Â 
Data	 Â scrapped	 Â from	 Â http://www.reuters.com/	 Â 

	 Â 

Example	 Â of	 Â Headline	 Â and	 Â story	 Â from	 Â Reuters	 Â Technology	 Â News	 Â 
	 Â 
Screenshots	 Â of	 Â Yahoo	 Â Finance	 Â 
	 Â 

	 Â 

	 Â 

	 Â 

Example	 Â of	 Â screen	 Â shot	 Â from	 Â yahoo	 Â 
finance.	 Â This	 Â shows	 Â the	 Â ticker	 Â for	 Â the	 Â 
NASDAQ	 Â Composite.	 Â We	 Â also	 Â 
experimented	 Â with	 Â others	 Â such	 Â as	 Â the	 Â 
Dow	 Â Jones	 Â Industrial	 Â Average	 Â and	 Â the	 Â 
Nikkei	 Â Index.	 Â However,	 Â it	 Â appears	 Â that	 Â 
the	 Â news	 Â we	 Â were	 Â scraping	 Â (ie,	 Â 
technology	 Â news)	 Â were	 Â better	 Â 
predictors	 Â of	 Â the	 Â NASDAQ	 Â Composite	 Â 
due	 Â to	 Â the	 Â large	 Â number	 Â of	 Â technology	 Â 
companies	 Â in	 Â this	 Â stock	 Â market	 Â index.	 Â 

	 Â 

6	 Â 

