Parallel	 Â learning	 Â of	 Â content	 Â 
recommendations	 Â using	 Â map-Â­â€reduce	 Â 
	 Â 
M i c h a e l 	 Â  P e r c y 	 Â  <m p e r c y@ c s . s t a n f o r d . e d u > 	 Â 
S t a n f o r d 	 Â U n i v e r s i t y 	 Â 
In	 Â  this	 Â  paper,	 Â  machine	 Â  learning	 Â  within	 Â  the	 Â  map-Â­â€reduce	 Â  paradigm	 Â  for	 Â 
ranking	 Â online	 Â advertisements	 Â  is	 Â explored.	 Â Online	 Â Logistic	 Â Regression	 Â 
(OLR)	 Â  is	 Â  used	 Â  to	 Â  predict	 Â  click-Â­â€through-Â­â€rates	 Â  on	 Â  advertising	 Â  data.	 Â  A	 Â 
Abstract	 Â 
parallelized	 Â  stochastic	 Â  gradient	 Â  descent	 Â  method	 Â  is	 Â  used	 Â  in	 Â  order	 Â  to	 Â 
split	 Â  the	 Â  dataset	 Â  across	 Â  multiple	 Â  machines,	 Â  and	 Â  the	 Â  resulting	 Â 
predictions	 Â are	 Â evaluated.	 Â 
	 Â 
The	 Â  content	 Â  ranking	 Â  and	 Â  recommendation	 Â  problem	 Â  is	 Â  a	 Â  common	 Â  one	 Â  in	 Â  industry,	 Â 
particularly	 Â in	 Â the	 Â realm	 Â of	 Â advertising.	 Â As	 Â the	 Â desired	 Â number	 Â of	 Â features	 Â to	 Â be	 Â used	 Â 
for	 Â  ranking	 Â  grows,	 Â  and	 Â  the	 Â  number	 Â  of	 Â  items	 Â  needing	 Â  to	 Â  be	 Â  learned	 Â  also	 Â  increases,	 Â 
Introduction	 Â 
larger	 Â  and	 Â  larger	 Â  numbers	 Â  of	 Â  training	 Â  examples	 Â  are	 Â  relied	 Â  upon	 Â  in	 Â  order	 Â  to	 Â 
accurately	 Â  predict	 Â  appropriate	 Â  contextual	 Â  rankings	 Â  of	 Â  them.	 Â  At	 Â  the	 Â  scale	 Â  commonly	 Â 
seen	 Â  in	 Â  production	 Â  systems	 Â  today,	 Â  parallelism	 Â  is	 Â  required	 Â  in	 Â  order	 Â  to	 Â  process	 Â  such	 Â 
large	 Â  amounts	 Â  of	 Â  data	 Â  in	 Â  a	 Â  reasonable	 Â  timeframe.	 Â  Learning	 Â  at	 Â  such	 Â  a	 Â  large	 Â  scale	 Â 
requires	 Â  that	 Â  the	 Â  learning	 Â  process	 Â  be	 Â  split	 Â  across	 Â  multiple	 Â  machines	 Â  and	 Â 
computation	 Â  parallelized.	 Â  A	 Â  very	 Â  common	 Â  approach	 Â  to	 Â  parallelization	 Â  is	 Â  the	 Â  map-Â­â€
reduce	 Â  paradigm,	 Â which	 Â  has	 Â  an	 Â  open	 Â  source	 Â  implementation	 Â  as	 Â  part	 Â  of	 Â  the	 Â  Hadoop	 Â 
project.	 Â  The	 Â  goal	 Â  of	 Â  this	 Â  project	 Â  is	 Â  to	 Â  implement	 Â  a	 Â  parallel	 Â  stochastic	 Â  gradient	 Â 
descent	 Â  (SGD)	 Â algorithm	 Â on	 Â  top	 Â of	 Â map-Â­â€reduce	 Â and	 Â apply	 Â  that	 Â algorithm	 Â  to	 Â  learning	 Â 
a	 Â contextual	 Â content	 Â ranking.	 Â 
The	 Â  data	 Â  set	 Â  being	 Â  used	 Â  is	 Â  the	 Â KDD	 Â  Cup	 Â  2012	 Â  track	 Â  2	 Â  (Predict	 Â the	 Â click-Â­â€through	 Â rate	 Â 
of	 Â  ads	 Â  given	 Â  the	 Â  query	 Â  and	 Â  user	 Â  information)	 Â  dataset	 Â  [1].	 Â  This	 Â  data	 Â  set	 Â  consists	 Â  of	 Â 
online	 Â  advertisement	 Â  click-Â­â€through	 Â  data	 Â  consisting	 Â  of	 Â  the	 Â  ad	 Â  id,	 Â  user	 Â  id,	 Â  number	 Â  of	 Â 
Data	 Â &	 Â Features	 Â 
impressions,	 Â  number	 Â  of	 Â  clicks,	 Â  and	 Â  various	 Â  other	 Â  features	 Â  including	 Â  contextual	 Â 
information	 Â  such	 Â  as	 Â  query	 Â  and	 Â  keyword	 Â  data.	 Â  The	 Â  data	 Â  set	 Â  is	 Â  not	 Â  huge	 Â  by	 Â  big	 Â  data	 Â 
standards,	 Â  but	 Â  still	 Â  sufficiently	 Â  large	 Â  as	 Â  to	 Â  benefit	 Â  from	 Â  parallelization,	 Â  at	 Â  10GB	 Â 
uncompressed	 Â (CSV)	 Â at	 Â 150M	 Â records	 Â in	 Â the	 Â training	 Â set.	 Â 
	 Â The	 Â  following	 Â  features	 Â  from	 Â  the	 Â  data	 Â  set	 Â  were	 Â  used	 Â  for	 Â  modeling	 Â  the	 Â  data:	 Â  user	 Â 
gender,	 Â  user	 Â  age	 Â  group	 Â  (6	 Â  groups	 Â  plus	 Â  unknown),	 Â  and	 Â  ad	 Â  position	 Â  (cross	 Â  product	 Â 
feature	 Â  of	 Â  depth	 Â  x	 Â  location).	 Â  These	 Â  features	 Â  were	 Â  chosen	 Â  because	 Â  they	 Â  are	 Â  very	 Â 

common	 Â  and	 Â  widely	 Â  available	 Â  ad	 Â  targeting	 Â  features	 Â  in	 Â  industry,	 Â  required	 Â  less	 Â  pre-Â­â€
processing	 Â  to	 Â  expose	 Â  as	 Â  feature	 Â  vectors,	 Â  and	 Â  seemed	 Â  like	 Â  obvious	 Â  choices	 Â  for	 Â 
predicting	 Â who	 Â might	 Â click	 Â on	 Â a	 Â given	 Â advertisement.	 Â 
The	 Â  software	 Â  written	 Â  as	 Â  part	 Â  of	 Â  this	 Â  project	 Â  was	 Â  built	 Â  using	 Â  the	 Â  Apache	 Â  Crunch	 Â  [3]	 Â 
map-Â­â€reduce	 Â  framework.	 Â  Crunch	 Â  is	 Â  an	 Â  implementation	 Â  of	 Â  the	 Â  FlumeJava	 Â  [4]	 Â 
framework,	 Â  which	 Â  came	 Â  out	 Â  of	 Â  Google.	 Â  It	 Â  provides	 Â  a	 Â  straightforward	 Â  programming	 Â 
Building	 Â Blocks	 Â 
model	 Â  that	 Â  abstracts	 Â  the	 Â  MapReduce	 Â  paradigm,	 Â  providing	 Â  a	 Â  programming	 Â  model	 Â 
that	 Â  allows	 Â  for	 Â  defining	 Â  pipelines	 Â  that	 Â  allow	 Â  for	 Â  shuffling,	 Â  sorting,	 Â  grouping,	 Â  and	 Â 
parallel	 Â  execution	 Â  of	 Â  mapper	 Â  and	 Â  reducer	 Â  tasks,	 Â  all	 Â  within	 Â  a	 Â  hybrid	 Â  functional	 Â  /	 Â 
object-Â­â€oriented	 Â paradigm	 Â in	 Â the	 Â Java	 Â language.	 Â 
	 Â Another	 Â  open	 Â  source	 Â  software	 Â  project,	 Â  which	 Â  aims	 Â  to	 Â  provide	 Â  building	 Â  blocks	 Â  for	 Â 
implementing	 Â  scalable	 Â machine	 Â  learning	 Â  algorithms,	 Â  is	 Â  Apache	 Â  Mahout	 Â  [5].	 Â  Mahout	 Â 
provides	 Â  several	 Â  types	 Â  of	 Â  out-Â­â€of-Â­â€the-Â­â€box	 Â machine	 Â  learning	 Â  algorithms,	 Â  one	 Â  of	 Â which	 Â 
is	 Â  a	 Â  general	 Â  online	 Â 
logistic	 Â  regression	 Â  (OLR)	 Â 
implementation.	 Â  Mahoutâ€™s	 Â 
implementation	 Â of	 Â OLR	 Â  is	 Â  a	 Â  single-Â­â€machine	 Â  in-Â­â€memory	 Â  implementation.	 Â As	 Â  such,	 Â no	 Â 
features	 Â  are	 Â  provided	 Â  to	 Â  attempt	 Â  to	 Â  split	 Â  its	 Â  stochastic	 Â  gradient	 Â  descent	 Â  method	 Â 
across	 Â multiple	 Â machines.	 Â 
In	 Â  order	 Â  for	 Â  a	 Â machine	 Â  learning	 Â  algorithm	 Â  to	 Â  fully	 Â  reap	 Â  the	 Â  benefits	 Â  of	 Â  running	 Â  on	 Â  a	 Â 
map-Â­â€reduce	 Â  cluster,	 Â a	 Â method	 Â  for	 Â parallelizing	 Â  the	 Â optimization	 Â  step	 Â  is	 Â desired.	 Â One	 Â 
method	 Â  for	 Â  parallelizing	 Â  stochastic	 Â  gradient	 Â  descent	 Â  is	 Â  described	 Â  in	 Â  Zinkevich,	 Â  et	 Â  al	 Â 
Parallel	 Â Stochastic	 Â Gradient	 Â Descent	 Â 
paper	 Â from	 Â NIPS	 Â 2010	 Â [2].	 Â The	 Â method	 Â is,	 Â as	 Â the	 Â authors	 Â say,	 Â â€œstrikingly	 Â simple.â€	 Â 
	 Â First,	 Â  a	 Â  number	 Â  of	 Â  splits	 Â  k	 Â  is	 Â  determined.	 Â  The	 Â  dataset	 Â  is	 Â  randomized	 Â  and	 Â  each	 Â 
machine	 Â  is	 Â  given	 Â  an	 Â  equal	 Â  portion	 Â  of	 Â  the	 Â  data	 Â  to	 Â  process.	 Â  Each	 Â  machine	 Â  runs	 Â 
stochastic	 Â  gradient	 Â  descent	 Â  (SGD)	 Â  in	 Â  parallel,	 Â  using	 Â  a	 Â  fixed	 Â  learning	 Â  rate.	 Â  Each	 Â 
machine	 Â  returns	 Â  the	 Â  learned	 Â  weight	 Â  vector	 Â  to	 Â  the	 Â  â€œmaster	 Â  routineâ€	 Â  which	 Â  collects	 Â 
those	 Â  weights	 Â  and	 Â  then	 Â  simple	 Â  averages	 Â  them	 Â  to	 Â  get	 Â  the	 Â  resulting	 Â  weight	 Â  vector	 Â 
which	 Â is	 Â used	 Â for	 Â classification.	 Â 
One	 Â way	 Â  of	 Â  addressing	 Â  the	 Â  online	 Â  advertising	 Â  problem	 Â  is	 Â  to	 Â  view	 Â  it	 Â  as	 Â  a	 Â  problem	 Â  of	 Â 
predicting	 Â  the	 Â  expected	 Â  click-Â­â€through-Â­â€rate	 Â  (CTR)	 Â  of	 Â  each	 Â  advertisement,	 Â  based	 Â  on	 Â 
what	 Â  is	 Â  known	 Â  about	 Â  the	 Â  user	 Â  viewing	 Â  it,	 Â  the	 Â  context	 Â  of	 Â  the	 Â  request,	 Â  and	 Â  the	 Â 
Ranking	 Â advertisements	 Â 
historical	 Â performance	 Â of	 Â the	 Â ad	 Â  itself.	 Â While	 Â multiple	 Â ads	 Â may	 Â be	 Â shown	 Â on	 Â a	 Â single	 Â 
page	 Â at	 Â one	 Â time,	 Â because	 Â the	 Â  inventory	 Â  is	 Â so	 Â  large	 Â â€“	 Â  in	 Â the	 Â case	 Â of	 Â the	 Â KDD	 Â cup	 Â data	 Â 
there	 Â  is	 Â  an	 Â  inventory	 Â  of	 Â  641,707	 Â  ads	 Â  â€“	 Â  an	 Â  assumption	 Â  of	 Â  independence	 Â  seems	 Â 
reasonable.	 Â Therefore,	 Â one	 Â way	 Â to	 Â approach	 Â the	 Â problem	 Â is	 Â to	 Â model	 Â the	 Â problem	 Â as	 Â 
solving	 Â  for	 Â ğ‘(ğ‘ğ‘™ğ‘–ğ‘ğ‘˜|ğ‘£ğ‘–ğ‘’ğ‘¤ , ğ‘ğ‘‘ğ‘–ğ‘‘ = ğ‘–)	 Â where	 Â  we	 Â  are	 Â  trying	 Â  to	 Â  find	 Â  the	 Â  probably	 Â  of	 Â  the	 Â 
user	 Â clicking	 Â on	 Â a	 Â given	 Â ad,	 Â assuming	 Â it	 Â was	 Â shown	 Â to	 Â them.	 Â 
	 Â 

In	 Â  order	 Â  to	 Â  implement	 Â  this	 Â  CTR-Â­â€prediction	 Â  model,	 Â  parallel	 Â  stochastic	 Â  gradient	 Â 
descent	 Â was	 Â  used	 Â  in	 Â  tandem	 Â with	 Â  online	 Â  logistic	 Â  regression	 Â  to	 Â  train	 Â models	 Â  against	 Â 
advertisements	 Â of	 Â varying	 Â numbers	 Â of	 Â examples.	 Â 
Implementation	 Â 
	 Â This	 Â  histogram	 Â  gives	 Â  a	 Â  sense	 Â  of	 Â  how	 Â  widely	 Â  the	 Â  data	 Â  set	 Â  varied,	 Â  in	 Â  terms	 Â  of	 Â 
impressions.	 Â There	 Â is	 Â a	 Â clear	 Â long-Â­â€tail	 Â pattern,	 Â where	 Â a	 Â handful	 Â of	 Â ads	 Â get	 Â millions	 Â of	 Â 
impressions,	 Â  a	 Â  significant	 Â  portion	 Â  get	 Â  hundreds	 Â  of	 Â  thousands,	 Â  and	 Â  many	 Â  get	 Â  only	 Â  a	 Â 
handful	 Â of	 Â impressions.	 Â 
	 Â 

	 Â 
	 Â Several	 Â  different	 Â  data	 Â  sizes	 Â  were	 Â  selected	 Â  for	 Â  modeling	 Â  in	 Â  order	 Â  to	 Â  get	 Â  a	 Â  sense	 Â  of	 Â 
how	 Â  the	 Â  parallelization	 Â  and	 Â  splitting	 Â  affected	 Â  prediction	 Â  accuracy.	 Â  In	 Â  each	 Â  case,	 Â  the	 Â 
total	 Â  data	 Â  set	 Â  was	 Â  split	 Â  into	 Â  a	 Â  training	 Â  set	 Â  (90%)	 Â  nd	 Â  a	 Â  test	 Â  set	 Â  (10%),	 Â  and	 Â  the	 Â  root	 Â 
mean	 Â squared	 Â error	 Â (RMSE)	 Â was	 Â used	 Â to	 Â evaluate	 Â the	 Â effectiveness	 Â of	 Â the	 Â algorithm.	 Â 
In	 Â  the	 Â  case	 Â  of	 Â  the	 Â  training	 Â  data,	 Â  the	 Â  data	 Â  was	 Â  processed	 Â  so	 Â  that	 Â  SGD	 Â  would	 Â  train	 Â 
against	 Â  discrete	 Â  click	 Â  /	 Â  no-Â­â€click	 Â  examples,	 Â  however	 Â when	 Â  evaluating	 Â  on	 Â  the	 Â  test	 Â  set,	 Â 
the	 Â  logistic	 Â model	 Â prediction	 Â was	 Â  compared	 Â  to	 Â  the	 Â  session	 Â CTR	 Â of	 Â  each	 Â user.	 Â  In	 Â  that	 Â 
way,	 Â we	 Â are	 Â able	 Â to	 Â treat	 Â the	 Â logistic	 Â regression	 Â output	 Â as	 Â predicting	 Â the	 Â CTR	 Â of	 Â each	 Â 
ad	 Â directly.	 Â 
	 Â Below	 Â  is	 Â a	 Â  table	 Â  showing	 Â how	 Â various	 Â  split	 Â  sizes	 Â affected	 Â  the	 Â prediction	 Â error	 Â of	 Â  the	 Â 
algorithm.	 Â Based	 Â on	 Â evaluating	 Â a	 Â wide	 Â range	 Â learning	 Â rates	 Â from	 Â 2	 Â to	 Â 256,	 Â a	 Â learning	 Â 
rate	 Â of	 Â 16	 Â was	 Â seen	 Â to	 Â be	 Â generally	 Â most	 Â effective	 Â across	 Â all	 Â data	 Â set	 Â sizes.	 Â Therefore,	 Â 
only	 Â  the	 Â  numbers	 Â  corresponding	 Â  to	 Â  a	 Â  learning	 Â  rate	 Â  of	 Â  16	 Â  are	 Â  presented	 Â  here.	 Â  We	 Â 
show	 Â how	 Â the	 Â train	 Â and	 Â test	 Â error	 Â rate	 Â varies	 Â as	 Â the	 Â number	 Â of	 Â splits	 Â are	 Â varied.	 Â 
	 Â 

Test	 Â RMSE	 Â 
Train	 Â RMSE	 Â 
Num.	 Â splits	 Â 
AdID	 Â 
0.0866869	 Â 
0.0026785	 Â 
0	 Â 
10616305	 Â 
0.0872241	 Â 
0.015221	 Â 
4	 Â 
10616305	 Â 
0.0872241	 Â 
0.015221	 Â 
64	 Â 
10616305	 Â 
0.2091639	 Â 
0.2121002	 Â 
0	 Â 
20001241	 Â 
0.2112489	 Â 
0.2144927	 Â 
4	 Â 
20001241	 Â 
0.2112489	 Â 
0.2144927	 Â 
64	 Â 
20001241	 Â 
0.1679808	 Â 
0.1312184	 Â 
4	 Â 
20157182	 Â 
0.1679808	 Â 
0.1312184	 Â 
64	 Â 
20157182	 Â 
0.168764	 Â 
0.1338959	 Â 
0	 Â 
20157182	 Â 
	 Â Looking	 Â  at	 Â  a	 Â  graph	 Â  of	 Â  the	 Â  test	 Â  error	 Â  data,	 Â  we	 Â  can	 Â  see	 Â  that	 Â  as	 Â  the	 Â  data	 Â  sets	 Â  grow,	 Â  it	 Â 
0.2041563	 Â 
0.183745	 Â 
0	 Â 
20192676	 Â 
0.205387	 Â 
0.1828694	 Â 
64	 Â 
20192676	 Â 
becomes	 Â more	 Â difficult	 Â to	 Â accurately	 Â predict	 Â the	 Â CTR.	 Â It	 Â seems	 Â  likely	 Â that	 Â that	 Â has	 Â to	 Â 
20192676	 Â 
4	 Â 
0.1828694	 Â 
0.2053871	 Â 
do	 Â  with	 Â  ads	 Â  getting	 Â  more	 Â  exposure	 Â  having	 Â  more	 Â  general	 Â  appeal,	 Â  whereas	 Â  ads	 Â  that	 Â 
get	 Â  less	 Â  exposure	 Â  may	 Â  be	 Â  more	 Â  likely	 Â  to	 Â  have	 Â  niche	 Â  appeal.	 Â  This	 Â  seems	 Â  indicative	 Â 
that	 Â the	 Â chosen	 Â  features	 Â are	 Â not	 Â sufficient	 Â to	 Â model	 Â the	 Â data,	 Â as	 Â we	 Â can	 Â see	 Â  from	 Â not	 Â 
only	 Â  the	 Â  test	 Â  error	 Â  going	 Â up,	 Â  as	 Â  larger	 Â numbers	 Â of	 Â  impressions	 Â  are	 Â used	 Â  for	 Â  training	 Â 
and	 Â  testing,	 Â  but	 Â  also	 Â  the	 Â  training	 Â  error	 Â  rising	 Â  as	 Â well.	 Â  It	 Â  seems	 Â  likely	 Â  that	 Â  additional	 Â 
content-Â­â€	 Â or	 Â context-Â­â€	 Â based	 Â features	 Â could	 Â achieve	 Â better	 Â results	 Â on	 Â the	 Â more	 Â popular	 Â 
ads.	 Â  Note	 Â  below,	 Â  in	 Â  this	 Â  graph,	 Â  that	 Â  the	 Â  rough	 Â  total	 Â  number	 Â  of	 Â  impressions	 Â  is	 Â  listed	 Â 
next	 Â  to	 Â each	 Â ad	 Â  in	 Â parenthesis.	 Â That	 Â  is	 Â  the	 Â  total	 Â number	 Â of	 Â  impressions	 Â  in	 Â  the	 Â entire	 Â 
data	 Â set,	 Â before	 Â splitting	 Â into	 Â train	 Â &	 Â test	 Â sets.	 Â 
	 Â 

	 Â 

	 Â 

As	 Â can	 Â be	 Â seen	 Â from	 Â looking	 Â at	 Â these	 Â graphs,	 Â the	 Â parallelization	 Â method	 Â proposed	 Â in	 Â 
[2]	 Â  appears	 Â  to	 Â  be	 Â  extremely	 Â  effective.	 Â  At	 Â  least,	 Â  it	 Â  does	 Â  not	 Â  appear	 Â  to	 Â  hurt	 Â  the	 Â  RMSE	 Â 
on	 Â  the	 Â  test	 Â  set	 Â  very	 Â  much.	 Â  In	 Â  the	 Â  case	 Â  of	 Â  the	 Â  1,000,000	 Â  impression	 Â  ad,	 Â  taking	 Â  the	 Â 
Conclusions	 Â 
parallelization	 Â  factor	 Â  from	 Â  0	 Â  (regular	 Â OLR)	 Â  to	 Â  64	 Â  only	 Â  increased	 Â  the	 Â RMSE	 Â  by	 Â  0.6%.	 Â 
On	 Â  the	 Â  smaller	 Â  (niche)	 Â  ads,	 Â  where	 Â  our	 Â  features	 Â  modeled	 Â  the	 Â  data	 Â  admirably,	 Â  there	 Â 
was	 Â also	 Â a	 Â negligible	 Â difference,	 Â and	 Â  in	 Â one	 Â case	 Â  the	 Â error	 Â rate	 Â was	 Â even	 Â reduced	 Â by	 Â 
the	 Â  parallelization.	 Â  It	 Â  seems	 Â  likely	 Â  that	 Â  that	 Â  can	 Â  be	 Â  attributed	 Â  to	 Â  some	 Â  kind	 Â  of	 Â 
regularization	 Â effect.	 Â 
	 Â From	 Â  a	 Â  practical	 Â  standpoint,	 Â  itâ€™s	 Â  also	 Â  worth	 Â  nothing	 Â  that	 Â  writing	 Â  and	 Â  debugging	 Â 
distributed	 Â  programs	 Â  is	 Â  harder	 Â  and	 Â  takes	 Â  longer	 Â  than	 Â  writing	 Â  single-Â­â€machine	 Â 
software	 Â  in	 Â  a	 Â  language	 Â  like	 Â  Matlab.	 Â  â€œBig	 Â  dataâ€	 Â  also	 Â  takes	 Â  a	 Â  lot	 Â  longer	 Â  to	 Â  munge	 Â  and	 Â 
pre-Â­â€process,	 Â and	 Â requires	 Â clever	 Â tricks	 Â to	 Â handle	 Â effectively.	 Â 
While	 Â this	 Â parallelization	 Â technique	 Â appears	 Â to	 Â work	 Â well	 Â on	 Â this	 Â data	 Â set,	 Â additional	 Â 
investigation	 Â would	 Â  be	 Â  helpful	 Â  in	 Â  order	 Â  to	 Â  establish	 Â  a	 Â  baseline	 Â  across	 Â  a	 Â wider	 Â  range	 Â 
of	 Â  data	 Â  sets	 Â  and	 Â  algorithms.	 Â  In	 Â  addition,	 Â  modeling	 Â  content	 Â  and	 Â  context	 Â  features,	 Â  in	 Â 
Future	 Â Work	 Â 
order	 Â  to	 Â  get	 Â  more	 Â  accuracy	 Â  on	 Â  more	 Â  general-Â­â€interest	 Â  advertising,	 Â  would	 Â  help	 Â  to	 Â 
validate	 Â  that	 Â  this	 Â  technique	 Â  scales	 Â  to	 Â  larger	 Â  feature	 Â  sets.	 Â  In	 Â  addition,	 Â  providing	 Â  this	 Â 
functionality	 Â  out	 Â  of	 Â  the	 Â  box	 Â  in	 Â  Crunch	 Â would	 Â  be	 Â  incredibly	 Â  useful	 Â  to	 Â  the	 Â  community	 Â 
at	 Â large.	 Â 
1.  2012	 Â  KDD	 Â  Cup	 Â  track	 Â  2.	 Â  Predict	 Â  the	 Â  click-Â­â€through	 Â  rate	 Â  of	 Â  ads	 Â  given	 Â  the	 Â  query	 Â 
and	 Â user	 Â information.	 Â http://www.kddcup2012.org/c/kddcup2012-Â­â€track2	 Â 
2.  Zinkevich,	 Â  M.,	 Â  Weimer,	 Â  M.,	 Â  Smola,	 Â  A.,	 Â  &	 Â  Li,	 Â  L.	 Â  (2010).	 Â  Parallelized	 Â  stochastic	 Â 
References	 Â 
gradient	 Â  descent.	 Â Advances	 Â  in	 Â Neural	 Â  Information	 Â  Processing	 Â  Systems,	 Â 23(23),	 Â 
1-Â­â€9.	 Â 
3.  Apache	 Â Crunch.	 Â http://incubator.apache.org/crunch/	 Â 
4.  Chambers,	 Â  C.,	 Â  Raniwala,	 Â  A.,	 Â  Perry,	 Â  F.,	 Â  Adams,	 Â  S.,	 Â  Henry,	 Â  R.	 Â  R.,	 Â  Bradshaw,	 Â  R.,	 Â  &	 Â 
Weizenbaum,	 Â  N.	 Â  (2010,	 Â  June).	 Â  FlumeJava:	 Â  easy,	 Â  efficient	 Â  data-Â­â€parallel	 Â 
pipelines.	 Â In	 Â ACM	 Â Sigplan	 Â Notices	 Â (Vol.	 Â 45,	 Â No.	 Â 6,	 Â pp.	 Â 363-Â­â€375).	 Â ACM.	 Â 
5.  Apache	 Â Mahout.	 Â http://mahout.apache.org/	 Â 
	 Â 

	 Â 

